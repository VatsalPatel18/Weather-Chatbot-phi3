from llama_cpp import Llama
import re
import json
import requests
from datetime import datetime, timedelta


api_key = 'c6dfc4d92a8f972d237ef696ec87b37a'


def get_weather(city):
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
    response = requests.get(url)
    return response.json() if response.status_code == 200 else None

def get_forecast(city):
    url = f"http://api.openweathermap.org/data/2.5/forecast?q={city}&appid={api_key}&units=metric"
    response = requests.get(url)
    return response.json() if response.status_code == 200 else None


def extract_intent_data(intent_response):
    json_pattern = re.compile(r'\{.*\}')
    json_match = json_pattern.search(intent_response)
    if json_match:
        intent_data_str = json_match.group()
        intent_data_str = intent_data_str.replace("'", "\"")
        try:
            return json.loads(intent_data_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
            return {"intent": None, "entities": {"city": None, "date": None}}
    return {"intent": None, "entities": {"city": None, "date": None}}

def fetch_weather_data(intent, city):
    if intent == "current_weather":
        return get_weather(city)
    elif intent == "forecast" or intent == 'forecast_weather':
        return get_forecast(city)
    return None

def parse_forecast(api_response, city):
    if not api_response:
        return "Sorry, I couldn't fetch the weather information."

    forecast_list = api_response.get('list', [])
    if not forecast_list:
        return f"Sorry, I couldn't fetch the weather information for {city}."

    tomorrow_date = (datetime.utcnow() + timedelta(days=1)).strftime('%Y-%m-%d')
    filtered_forecasts = [forecast for forecast in forecast_list if tomorrow_date in forecast['dt_txt']]
    
    if filtered_forecasts:
        selected_forecast = filtered_forecasts[0]
        forecast_temp = selected_forecast['main']['temp']
        forecast_weather = selected_forecast['weather'][0]['description']
        wind_speed = selected_forecast['wind']['speed']
        humidity = selected_forecast['main']['humidity']

        return (
            f"The forecast for {city} on {tomorrow_date} is {forecast_weather} with a temperature of {forecast_temp}°C, "
            f"wind speed of {wind_speed} meters per second, and humidity of {humidity}%."
        )
    return f"Sorry, I couldn't fetch the weather information for {city} for tomorrow."

def parse_current_weather(api_response, city):
    if not api_response:
        return "Sorry, I couldn't fetch the weather information."

    main = api_response.get('main', {})
    weather = api_response.get('weather', [{}])[0]
    wind = api_response.get('wind', {})
    
    temp = main.get('temp')
    description = weather.get('description')
    wind_speed = wind.get('speed')
    humidity = main.get('humidity')

    return (
        f"The current weather in {city} is {description} with a temperature of {temp}°C, "
        f"wind speed of {wind_speed} meters per second, and humidity of {humidity}%."
    )




def generate_assistant_response(user_input, api_response, model):
    prompt = f"Human: {user_input}\nAPI: {api_response}\nAssistant:"
    output = model(prompt, max_tokens=256, stop=["Human:", "Assistant:"])
    response_text = output["choices"][0]["text"].strip()
    
    # Extract only the relevant part
    return extract_relevant_part(response_text)

def extract_relevant_part(response_text):
    pattern = re.compile(r'The (current )?weather in .+ is .+ with a temperature of .+°C, wind speed of .+ meters per second, and humidity of .+%.')
    match = pattern.search(response_text)
    if match:
        return match.group()
    return response_text


def inference_pipeline(user_input, model):
    # Step 1: Intent Identification
    prompt = f"Human: {user_input}\nAssistant (please respond in JSON format):"
    intent_output = model(prompt, max_tokens=64, stop=["Human:", "Assistant:"])
    intent_response = intent_output["choices"][0]["text"].strip()

    # Extract intent data
    intent_data = extract_intent_data(intent_response)
    intent = intent_data.get("intent")
    city = intent_data.get("entities", {}).get("city")
    date = intent_data.get("entities", {}).get("date")

    if not intent or not city:
        return "Sorry, I couldn't understand the query."

    # Step 2: Fetch weather data from API
    api_response = fetch_weather_data(intent, city)
    if intent == "current_weather":
        parsed_response = parse_current_weather(api_response, city)
    else:
        parsed_response = parse_forecast(api_response, city)

    # Step 3: Generate the final assistant response using LLM
    final_response = generate_assistant_response(user_input, parsed_response, model)
    return final_response


llm = Llama(
    model_path="../models/gguf/-unsloth.F16.gguf",  # Path to your GGUF model
    n_ctx=4096,  # The max sequence length to use
    n_threads=8, # Number of CPU threads to use
    n_gpu_layers=0, # Number of layers to offload to GPU
)


user_input = "What is the weather in Kalyani on Today?"
final_response = inference_pipeline(user_input, llm)
print("Final Assistant Response:", final_response)


user_input = "What is the weather in Kalyani on Today?"
final_response = inference_pipeline(user_input, model, tokenizer)
print("Final Assistant Response:", final_response)


final_response



