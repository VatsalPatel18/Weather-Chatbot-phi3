from llama_cpp import Llama

# Initialize the Llama model
model_path = "../models/gguf/-unsloth.F16.gguf"  # Update this to your actual model path
llm = Llama(
    model_path=model_path,
    n_ctx=4096,
    n_threads=8,
    n_gpu_layers=0,
)






import requests
api_key = 'c6dfc4d92a8f972d237ef696ec87b37a'
def get_weather(city):
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
    response = requests.get(url)
    return response.json() if response.status_code == 200 else None

def get_forecast(city):
    url = f"http://api.openweathermap.org/data/2.5/forecast?q={city}&appid={api_key}&units=metric"
    response = requests.get(url)
    return response.json() if response.status_code == 200 else None


import json
import re
from datetime import datetime, timedelta
from llama_cpp import Llama

# Function to extract intent data from the response
def extract_intent_data(intent_response):
    json_pattern = re.compile(r'\{.*\}')
    json_match = json_pattern.search(intent_response)
    if json_match:
        intent_data_str = json_match.group()
        intent_data_str = intent_data_str.replace("'", "\"")
        try:
            return json.loads(intent_data_str)
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
    return {}

# Function to fetch weather data based on intent
def fetch_weather_data(intent, city):
    if intent == "current_weather":
        return get_weather(city)
    elif intent == "forecast" or intent == 'forecast_weather':
        return get_forecast(city)
    return None

# Function to parse forecast data
def parse_forecast(api_response, city, date):
    if not api_response:
        return "Sorry, I couldn't fetch the weather information."

    forecast_list = api_response.get('list', [])
    if not forecast_list:
        return f"Sorry, I couldn't fetch the weather information for {city}."

    filtered_forecasts = [forecast for forecast in forecast_list if date in forecast['dt_txt']]
    
    if filtered_forecasts:
        selected_forecast = filtered_forecasts[0]
        forecast_temp = selected_forecast['main']['temp']
        forecast_weather = selected_forecast['weather'][0]['description']
        wind_speed = selected_forecast['wind']['speed']
        humidity = selected_forecast['main']['humidity']

        return (
            f"The forecast for {city} on {date} is {forecast_weather} with a temperature of {forecast_temp}°C, "
            f"wind speed of {wind_speed} meters per second, and humidity of {humidity}%."
        )
    return f"Sorry, I couldn't fetch the weather information for {city} on {date}."

# Function to parse current weather data
def parse_current_weather(api_response, city):
    if not api_response:
        return "Sorry, I couldn't fetch the weather information."
        
    print(api_response)
    main = api_response.get('main', {})
    weather = api_response.get('weather', [{}])[0]
    wind = api_response.get('wind', {})
    
    temp = main.get('temp')
    description = weather.get('description')
    wind_speed = wind.get('speed')
    humidity = main.get('humidity')

    return (
        f"The current weather in {city} is {description} with a temperature of {temp}°C, "
        f"wind speed of {wind_speed} meters per second, and humidity of {humidity}%."
    )

# Function to generate assistant response using Llama model
def generate_assistant_response(user_input, api_response, llm):
    prompt = f"<s>\nUser: {user_input}\nAssistant:\n{api_response}\n"
    output = llm(
        prompt,
        max_tokens=64,
        stop=["\n"],
        echo=True,
    )
    response_text = output['choices'][0]['text']
    return extract_relevant_part(response_text)

# Function to extract relevant part from the response
def extract_relevant_part(response_text):
    pattern = re.compile(r'The (current )?weather in .+ is .+ with a temperature of .+°C, wind speed of .+ meters per second, and humidity of .+%.')
    match = pattern.search(response_text)
    if match:
        return match.group()
    return response_text

# Inference pipeline function
def inference_pipeline(user_input, llm):
    # Step 1: Intent Identification
    intent_prompt = f"<s>\n{user_input}\n"
    intent_output = llm(
        intent_prompt,
        max_tokens=64,
        stop=["\n"],
        echo=True,
    )
    intent_response = intent_output['choices'][0]['text']

    # Extract intent data
    intent_data = extract_intent_data(intent_response)
    intent = intent_data.get("intent")
    city = intent_data.get("entities", {}).get("city")
    date = intent_data.get("entities", {}).get("date")

    print('Extracted Intent: {} City : {} ; Data : {} '.format(intent,city,date))

    # Step 2: Fetch weather data from API
    api_response = fetch_weather_data(intent, city)
    if intent == "current_weather":
        parsed_response = parse_current_weather(api_response, city)
    else:
        parsed_response = parse_forecast(api_response, city, date)

    # Step 3: Generate the final assistant response using LLM
    final_response = generate_assistant_response(user_input, parsed_response, llm)
    return final_response

# Example user input
user_input = "What is the weather in Kolkata today?"
final_response = inference_pipeline(user_input, llm)
print("Final Assistant Response:", final_response)






user_input = "What is the weather in Kalyani in the Evening ?"
final_response = inference_pipeline(user_input, llm)
print("Final Assistant Response:", final_response)


user_input = "What is the weather in London in tommorow ?"
final_response = inference_pipeline(user_input, llm)
print("Final Assistant Response:", final_response)



