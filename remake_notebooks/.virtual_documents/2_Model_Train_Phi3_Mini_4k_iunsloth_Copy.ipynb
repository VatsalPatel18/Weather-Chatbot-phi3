!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps xformers "trl<0.9.0" peft accelerate bitsandbytes


!pip install -U scikit-learn


!pip install accelerate


!pip install tensorboard


!pip install -U flash-attn


from unsloth import FastLanguageModel
import torch
import os
import json
from datasets import load_dataset, Dataset
from transformers import TrainingArguments
from trl import SFTTrainer
from unsloth.chat_templates import get_chat_template
from transformers import TextStreamer
import requests
from transformers import TextStreamer
from unsloth.chat_templates import get_chat_template
import random
from datasets import load_metric
from sklearn.model_selection import train_test_split
from transformers.integrations import TensorBoardCallback


# from accelerate import Accelerator


# accelerator = Accelerator()


# accelerator = Accelerator(gradient_accumulation_steps=4)
# device = accelerator.device


# Settings
max_seq_length = 512 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.


!nvidia-smi


# torch.cuda.empty_cache()


model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Phi-3-mini-4k-instruct",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    device_map="auto"
)


model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)


# !git clone https://github.com/VatsalPatel18/Weather-Chatbot-phi3.git


from datasets import Dataset, DatasetDict
dataset_path = './data/weather_chatbot_dataset1.json'
with open(dataset_path, 'r') as f:
    weather_data = json.load(f)


weather_data[0]


weather_data[5]


weather_data[11]


# Split the dataset
intent_identification_data = []
assistant_response_data = []

for entry in weather_data:
    # For intent identification
    intent_identification_data.append({
        "user_input": entry["user_input"],
        "intent_extraction": entry["intent_extraction"]
    })

    # For assistant response generation
    assistant_response_data.append({
        "user_input": entry["user_input"],
        "api_response": entry["api_response"],
        "assistant_response": entry["assistant_response"]
    })

# Combine the datasets
combined_data = intent_identification_data + assistant_response_data

# Shuffle the combined dataset
random.shuffle(combined_data)

# Convert to Hugging Face Dataset format
combined_dataset = Dataset.from_dict({"conversations": combined_data})
dataset_split = combined_dataset.train_test_split(test_size=0.2)
train_dataset = dataset_split['train']
val_dataset = dataset_split['test']


train_dataset


train_dataset[0]


from unsloth.chat_templates import get_chat_template
from typing import Dict, List








def get_chat_template(tokenizer, chat_template="phi-3", mapping={"role": "from", "content": "value", "user": "human", "assistant": "gpt", "API": "api"}):
    return tokenizer

tokenizer = get_chat_template(
    tokenizer,
    chat_template="phi-3",
    mapping={"role": "from", "content": "value", "user": "human", "assistant": "gpt", "API": "api"},
)

def format_combined_prompts(batch: Dict[str, List[Dict]], tokenizer):
    def format_conversation(example: Dict[str, str]):
        conversation = [{"from": "human", "value": example['user_input']}]
        if "intent_extraction" in example and example['intent_extraction'] is not None:
            conversation.append({"from": "gpt", "value": str(example['intent_extraction'])})
        if "api_response" in example and example['api_response'] is not None:
            conversation.append({"from": "api", "value": str(example['api_response'])})
            if "assistant_response" in example and example['assistant_response'] is not None:
                conversation.append({"from": "gpt", "value": example['assistant_response']})
        return conversation

    texts = [tokenizer.apply_chat_template(format_conversation(example), tokenize=False, add_generation_prompt=True) for example in batch["conversations"]]
    return {"text": texts}

# Apply formatting
train_dataset = train_dataset.map(lambda batch: format_combined_prompts(batch, tokenizer), batched=True)
val_dataset = val_dataset.map(lambda batch: format_combined_prompts(batch, tokenizer), batched=True)



train_dataset[0]


train_dataset[1]


# Modify global metric lists to include validation split
train_metrics = {"step": [], "bleu": [], "rouge": [], "perplexity": []}
val_metrics = {"step": [], "bleu": [], "rouge": [], "perplexity": []}

def compute_metrics(eval_pred, split="train"):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # BLEU
    bleu = load_metric("bleu")
    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)

    # ROUGE
    rouge = load_metric("rouge")
    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)

    # Perplexity
    perplexity = torch.exp(torch.tensor(eval_pred[0].mean()))

    metrics = {
        "bleu": bleu_result['bleu'],
        "rouge": rouge_result['rougeL'].mid.fmeasure,
        "perplexity": perplexity.item(),
    }

    if split == "train":
        train_metrics["step"].append(len(train_metrics["step"]) + 1)
        train_metrics["bleu"].append(metrics["bleu"])
        train_metrics["rouge"].append(metrics["rouge"])
        train_metrics["perplexity"].append(metrics["perplexity"])
    else:
        val_metrics["step"].append(len(val_metrics["step"]) + 1)
        val_metrics["bleu"].append(metrics["bleu"])
        val_metrics["rouge"].append(metrics["rouge"])
        val_metrics["perplexity"].append(metrics["perplexity"])

    return metrics


# Training arguments
training_args = TrainingArguments(
    per_device_train_batch_size=2,
    gradient_accumulation_steps=2,
    warmup_steps=5,
    max_steps=30,
    learning_rate=2e-4,
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    logging_steps=1,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=3407,
    output_dir="outputs_combined",
    logging_dir='./logs',
    report_to="tensorboard",
)


# train_metrics


trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    dataset_text_field="text",
    max_seq_length=512,
    dataset_num_proc=2,
    packing=False,
    args=training_args,
    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, split="train"),
)


# trainer = SFTTrainer(
#     model=accelerator.prepare(model),
#     tokenizer=tokenizer,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     dataset_text_field="text",
#     max_seq_length=max_seq_length,
#     dataset_num_proc=2,
#     packing=False,
#     args=training_args,
#     compute_metrics=lambda eval_pred: compute_metrics(eval_pred, split="train"),
# )


# Add TensorBoard callback
trainer.add_callback(TensorBoardCallback)


# Train the model
trainer_stats = trainer.train()


#torch.cuda.empty()


torch.cuda.empty_cache()


# trainer.evaluate()


# val_results = trainer.evaluate(eval_dataset=val_dataset, metric_key_prefix="val")


model.push_to_hub("VatsalPatel18/phi3-mini-WeatherBot", token=os.getenv("HUGGINGFACE_HUB_TOKEN"))


model.save_pretrained("../models/trained")
tokenizer.save_pretrained("../models/trained")


# !pip install matplotlib


model.save_pretrained_gguf("../models/gguf/", tokenizer, quantization_method = "q4_k_m")


# import matplotlib.pyplot as plt
# import pandas as pd

# def plot_metrics(metrics, title, ylabel):
#     df = pd.DataFrame(metrics)
#     plt.figure(figsize=(10, 6))
#     for column in metrics:
#         if column != "step":
#             plt.plot(df["step"], df[column], label=column)
#     plt.xlabel("Step")
#     plt.ylabel(ylabel)
#     plt.title(title)
#     plt.legend()
#     plt.show()

# # Plotting training metrics
# plot_metrics(train_metrics, "Training Metrics", "Score")

# # Plotting validation metrics
# plot_metrics(val_metrics, "Validation Metrics", "Score")




# from unsloth import FastLanguageModel

# model = FastLanguageModel.from_pretrained("path/to/save/model")
# tokenizer = AutoTokenizer.from_pretrained("path/to/save/tokenizer")




