{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9097f8a2-6394-4d53-bd2f-4c6ede9717b3",
      "metadata": {
        "scrolled": true,
        "id": "9097f8a2-6394-4d53-bd2f-4c6ede9717b3",
        "outputId": "25c8114e-9d14-4e35-88a2-93e128ee6974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-s0yur5yg/unsloth_416e9ce024f74e4691d79b2178a50668\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-s0yur5yg/unsloth_416e9ce024f74e4691d79b2178a50668\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 8a9e24ed4f092e438bd09436a897f2ca2530e72b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tyro (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.41.2)\n",
            "Collecting datasets>=2.16.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.25.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.6-py3-none-any.whl size=115724 sha256=435295a0e4a644efc94dee68851ca485a03cc022286dcbeda7f8159d620d43b0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5gh0xg54/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: xxhash, unsloth, shtab, requests, dill, multiprocess, tyro, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 multiprocess-0.70.16 requests-2.32.3 shtab-1.7.1 tyro-0.8.4 unsloth-2024.6 xxhash-3.4.1\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl<0.9.0\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, xformers, trl, peft, accelerate\n",
            "Successfully installed accelerate-0.31.0 bitsandbytes-0.43.1 peft-0.11.1 trl-0.8.6 xformers-0.0.26.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fae2edfa-163e-439f-a906-775fadb3d2c7",
      "metadata": {
        "id": "fae2edfa-163e-439f-a906-775fadb3d2c7",
        "outputId": "080feca8-e319-4662-8e1c-2c88dfaa30dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "nDNK24M7-Ygb",
        "outputId": "0219884b-3cf0-401b-b1ae-e5149d9a0ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nDNK24M7-Ygb",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bf8668a6-2b3e-4cbb-ae94-b2cccf9fda7b",
      "metadata": {
        "id": "bf8668a6-2b3e-4cbb-ae94-b2cccf9fda7b",
        "outputId": "202eb9c0-1277-4cf9-bf1f-c89ca4c69800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U flash-attn"
      ],
      "metadata": {
        "id": "D4-mjigTH1r-",
        "outputId": "4a5e891b-4465-4108-cfc4-91663c465652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "D4-mjigTH1r-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.3.0+cu121)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120889689 sha256=5022ba11d48bf74926da9c16260f4ea2b9bb7f4e29bdb4bd6e1383ad1c55d16f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: einops, flash-attn\n",
            "Successfully installed einops-0.8.0 flash-attn-2.5.9.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1a31650b-4a44-463d-b97d-8abab6357315",
      "metadata": {
        "id": "1a31650b-4a44-463d-b97d-8abab6357315"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import TextStreamer\n",
        "import requests\n",
        "from transformers import TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import random\n",
        "from datasets import load_metric\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers.integrations import TensorBoardCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "GpWzdqfd3-N8"
      },
      "id": "GpWzdqfd3-N8",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator = Accelerator()"
      ],
      "metadata": {
        "id": "MYoO33dvMD13"
      },
      "id": "MYoO33dvMD13",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator = Accelerator(gradient_accumulation_steps=4)\n",
        "device = accelerator.device"
      ],
      "metadata": {
        "id": "oxjZnZnt-XCI"
      },
      "id": "oxjZnZnt-XCI",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4d318eca-630a-4467-b81f-64b1980eaff0",
      "metadata": {
        "id": "4d318eca-630a-4467-b81f-64b1980eaff0"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3fb08d31-4fe6-431c-9679-b0a419847d32",
      "metadata": {
        "id": "3fb08d31-4fe6-431c-9679-b0a419847d32",
        "outputId": "0df2eeb3-e97f-41d9-c173-5c7c5a26ca42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 12 20:41:38 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "A2e8ElFuD2ww"
      },
      "id": "A2e8ElFuD2ww",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a0de2c83-6056-4a2f-bab9-111963a2d59f",
      "metadata": {
        "id": "a0de2c83-6056-4a2f-bab9-111963a2d59f",
        "outputId": "3c892177-c181-4c6e-a589-a4ea43ce3f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Mistral patching release 2024.6\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "44e32e83-beca-470f-a30b-845ac67b40a1",
      "metadata": {
        "id": "44e32e83-beca-470f-a30b-845ac67b40a1",
        "outputId": "a055e6bc-6061-41b5-c264-433973b82fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VatsalPatel18/Weather-Chatbot-phi3.git"
      ],
      "metadata": {
        "id": "YNQHOOee6PlI",
        "outputId": "1a785670-478e-4a22-d270-1e7a46053249",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YNQHOOee6PlI",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Weather-Chatbot-phi3' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7681fea0-f92a-4b7b-b389-95edf91acb35",
      "metadata": {
        "id": "7681fea0-f92a-4b7b-b389-95edf91acb35"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "dataset_path = '/content/Weather-Chatbot-phi3/remake_notebooks/data/weather_chatbot_dataset.json'\n",
        "with open(dataset_path, 'r') as f:\n",
        "    weather_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fd84fe32-7964-4698-8eef-935d10ad0437",
      "metadata": {
        "id": "fd84fe32-7964-4698-8eef-935d10ad0437",
        "outputId": "430cebf7-6388-4249-e382-8a9f1db84476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_input': 'What is the current weather in Frankfurt today?',\n",
              " 'intent_extraction': {'intent': 'current_weather',\n",
              "  'entities': {'city': 'Frankfurt', 'date': 'today'}},\n",
              " 'api_response': {'location': 'Frankfurt, DE',\n",
              "  'temperature': 12.76,\n",
              "  'description': 'clear sky',\n",
              "  'wind_speed': 7.2,\n",
              "  'humidity': 62},\n",
              " 'assistant_response': 'The weather in Frankfurt is currently clear sky with a temperature of 12.76°C, wind speed of 7.2 meters per second, and humidity of 62%.'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "weather_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f449e4b6-b5b7-43e0-93a8-c9d625701c0c",
      "metadata": {
        "id": "f449e4b6-b5b7-43e0-93a8-c9d625701c0c"
      },
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "intent_identification_data = []\n",
        "assistant_response_data = []\n",
        "\n",
        "for entry in weather_data:\n",
        "    # For intent identification\n",
        "    intent_identification_data.append({\n",
        "        \"user_input\": entry[\"user_input\"],\n",
        "        \"intent_extraction\": entry[\"intent_extraction\"]\n",
        "    })\n",
        "\n",
        "    # For assistant response generation\n",
        "    assistant_response_data.append({\n",
        "        \"user_input\": entry[\"user_input\"],\n",
        "        \"api_response\": entry[\"api_response\"],\n",
        "        \"assistant_response\": entry[\"assistant_response\"]\n",
        "    })\n",
        "\n",
        "# Combine the datasets\n",
        "combined_data = intent_identification_data + assistant_response_data\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "random.shuffle(combined_data)\n",
        "# train_test_split\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "combined_dataset = Dataset.from_dict({\"conversations\": combined_data})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fbe1353c-fa76-4fd1-9944-1dadcf660c88",
      "metadata": {
        "id": "fbe1353c-fa76-4fd1-9944-1dadcf660c88"
      },
      "outputs": [],
      "source": [
        "dataset_split = combined_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = dataset_split['train']\n",
        "val_dataset = dataset_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5778f8c6-a20c-44f8-a089-959ba8c5fcca",
      "metadata": {
        "id": "5778f8c6-a20c-44f8-a089-959ba8c5fcca"
      },
      "outputs": [],
      "source": [
        "# combined_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5b3b4e65-571d-44b6-85a9-21f468a090f4",
      "metadata": {
        "id": "5b3b4e65-571d-44b6-85a9-21f468a090f4"
      },
      "outputs": [],
      "source": [
        "# from datasets import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b4bb7b72-b528-4e4e-b319-8e24c6298e25",
      "metadata": {
        "id": "b4bb7b72-b528-4e4e-b319-8e24c6298e25"
      },
      "outputs": [],
      "source": [
        "# dataset_dict['intent_identification'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6593e2b7-7f15-478e-a548-2609e556852b",
      "metadata": {
        "id": "6593e2b7-7f15-478e-a548-2609e556852b"
      },
      "outputs": [],
      "source": [
        "# dataset_dict['assistant_response'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "574abc30-4cb6-475d-8a0f-b145388ac14c",
      "metadata": {
        "id": "574abc30-4cb6-475d-8a0f-b145388ac14c",
        "outputId": "92b2a377-1df3-492a-a4c2-b50b2f54cf82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "151fa84982b34bd2943c8710a3323230",
            "4b3254e94c074e5cbaf348873dd5eca3",
            "0a47a40627d942189c5bec1c09f18f5f",
            "799c860ac0c947e392446fe239609b74",
            "79c1b679c8ac48429256082179cbdaf1",
            "12e8388295e24026863747a6cb12d0e1",
            "4b1abcb8ceef48b2831e2223160d1525",
            "4453dd8db2c247e88eb1a76142b22b74",
            "01e60bb1c87849ee969c519b03e222dd",
            "a27bdcfc3d294bfc912113651b3dc382",
            "90122c6d58ac46e99c53c99df2b52344",
            "6a3d97bc82fd40c3b87ff891a57842a7",
            "cead9a5820994d81a5d728c7e1c57d6e",
            "546557f8d0d54fcaaa19fba886defa6d",
            "9b3c4f8d1c8d49aa97754044338cc32f",
            "8329a931b53d4171a089c0db4e27d43b",
            "3abf2836489c4d03bb5f6b93473c3d9d",
            "17e4d85de05b42cebf22a59a973ec6a7",
            "0912eb86282c43f28db42635cea77a79",
            "66ae625829c84d0fb74cb18c2ce58bd8",
            "3e3bef826ac24ddcadd426e506fd1b67",
            "6610b30247174f00b2acad688dd9655b"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1062 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "151fa84982b34bd2943c8710a3323230"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/266 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a3d97bc82fd40c3b87ff891a57842a7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "from typing import Dict, List\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"phi-3\",\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\", \"API\": \"api\"},\n",
        ")\n",
        "\n",
        "def format_combined_prompts(batch: Dict[str, List[Dict]], tokenizer):\n",
        "    def format_conversation(example: Dict[str, str]):\n",
        "        conversation = [{\"from\": \"human\", \"value\": example['user_input']}]\n",
        "        if \"intent_extraction\" in example and example['intent_extraction'] is not None:\n",
        "            conversation.append({\"from\": \"gpt\", \"value\": str(example['intent_extraction'])})\n",
        "        if \"api_response\" in example and example['api_response'] is not None:\n",
        "            conversation.append({\"from\": \"api\", \"value\": str(example['api_response'])})\n",
        "            if \"assistant_response\" in example and example['assistant_response'] is not None:\n",
        "                conversation.append({\"from\": \"gpt\", \"value\": example['assistant_response']})\n",
        "        return conversation\n",
        "\n",
        "    texts = [tokenizer.apply_chat_template(format_conversation(example), tokenize=False, add_generation_prompt=True) for example in batch[\"conversations\"]]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting\n",
        "train_dataset = train_dataset.map(lambda batch: format_combined_prompts(batch, tokenizer), batched=True)\n",
        "val_dataset = val_dataset.map(lambda batch: format_combined_prompts(batch, tokenizer), batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4f35072a-c06c-4daf-8e53-681dcfbeab20",
      "metadata": {
        "id": "4f35072a-c06c-4daf-8e53-681dcfbeab20",
        "outputId": "9b7d6383-7976-410a-c97f-7aa8b25f7876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conversations': {'api_response': None,\n",
              "  'assistant_response': None,\n",
              "  'intent_extraction': {'entities': {'city': 'Budapest', 'date': 'today'},\n",
              "   'intent': 'current_weather'},\n",
              "  'user_input': 'What is the current weather in Budapest today?'},\n",
              " 'text': \"<s><|user|>\\nWhat is the current weather in Budapest today?<|end|>\\n<|assistant|>\\n{'entities': {'city': 'Budapest', 'date': 'today'}, 'intent': 'current_weather'}<|end|>\\n<|assistant|>\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dc919810-2f1e-44f1-863d-2605c80b3b1f",
      "metadata": {
        "scrolled": true,
        "id": "dc919810-2f1e-44f1-863d-2605c80b3b1f",
        "outputId": "72cb9031-38f7-4407-8b76-e541deb512f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conversations': {'api_response': None,\n",
              "  'assistant_response': None,\n",
              "  'intent_extraction': {'entities': {'city': 'Toulouse', 'date': '2024-06-13'},\n",
              "   'intent': 'forecast_weather'},\n",
              "  'user_input': 'What is the forecast weather in Toulouse 2024-06-13?'},\n",
              " 'text': \"<s><|user|>\\nWhat is the forecast weather in Toulouse 2024-06-13?<|end|>\\n<|assistant|>\\n{'entities': {'city': 'Toulouse', 'date': '2024-06-13'}, 'intent': 'forecast_weather'}<|end|>\\n<|assistant|>\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b05e5613-ae79-4778-bc38-a20510bbb4c2",
      "metadata": {
        "id": "b05e5613-ae79-4778-bc38-a20510bbb4c2"
      },
      "outputs": [],
      "source": [
        "# Modify global metric lists to include validation split\n",
        "train_metrics = {\"step\": [], \"bleu\": [], \"rouge\": [], \"perplexity\": []}\n",
        "val_metrics = {\"step\": [], \"bleu\": [], \"rouge\": [], \"perplexity\": []}\n",
        "\n",
        "def compute_metrics(eval_pred, split=\"train\"):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU\n",
        "    bleu = load_metric(\"bleu\")\n",
        "    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # ROUGE\n",
        "    rouge = load_metric(\"rouge\")\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity = torch.exp(torch.tensor(eval_pred[0].mean()))\n",
        "\n",
        "    metrics = {\n",
        "        \"bleu\": bleu_result['bleu'],\n",
        "        \"rouge\": rouge_result['rougeL'].mid.fmeasure,\n",
        "        \"perplexity\": perplexity.item(),\n",
        "    }\n",
        "\n",
        "    if split == \"train\":\n",
        "        train_metrics[\"step\"].append(len(train_metrics[\"step\"]) + 1)\n",
        "        train_metrics[\"bleu\"].append(metrics[\"bleu\"])\n",
        "        train_metrics[\"rouge\"].append(metrics[\"rouge\"])\n",
        "        train_metrics[\"perplexity\"].append(metrics[\"perplexity\"])\n",
        "    else:\n",
        "        val_metrics[\"step\"].append(len(val_metrics[\"step\"]) + 1)\n",
        "        val_metrics[\"bleu\"].append(metrics[\"bleu\"])\n",
        "        val_metrics[\"rouge\"].append(metrics[\"rouge\"])\n",
        "        val_metrics[\"perplexity\"].append(metrics[\"perplexity\"])\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "54b08b0c-e1c7-4c44-a70c-2f9b7a9e2d3f",
      "metadata": {
        "id": "54b08b0c-e1c7-4c44-a70c-2f9b7a9e2d3f"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    #gradient_accumulation_steps=2,\n",
        "    warmup_steps=5,\n",
        "    max_steps=60,\n",
        "    learning_rate=2e-4,\n",
        "    #fp16=not torch.cuda.is_bf16_supported(),\n",
        "    fp16 = True,\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs_combined\",\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"tensorboard\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b5c7d3c5-bba8-4aec-a801-52401cf0d7ea",
      "metadata": {
        "id": "b5c7d3c5-bba8-4aec-a801-52401cf0d7ea"
      },
      "outputs": [],
      "source": [
        "# train_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "48dc6600-8028-424b-8793-c973a1c2098b",
      "metadata": {
        "id": "48dc6600-8028-424b-8793-c973a1c2098b"
      },
      "outputs": [],
      "source": [
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     dataset_text_field=\"text\",\n",
        "#     max_seq_length=max_seq_length,\n",
        "#     dataset_num_proc=2,\n",
        "#     packing=False,\n",
        "#     args=training_args,\n",
        "#     compute_metrics=lambda eval_pred: compute_metrics(eval_pred, split=\"train\"),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=accelerator.prepare(model),\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=training_args,\n",
        "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, split=\"train\"),\n",
        ")"
      ],
      "metadata": {
        "id": "6hxblAEx_WG_",
        "outputId": "c3f0632c-6f40-4252-d5ae-780ed2ec21f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "id": "6hxblAEx_WG_",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'AcceleratorState' object has no attribute 'mixed_precision'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-010fc5accfe4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer = SFTTrainer(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mmixed_precision\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmixed_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/state.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1077\u001b[0m             )\n\u001b[1;32m   1078\u001b[0m         \u001b[0;31m# Raise a typical AttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'AcceleratorState' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'AcceleratorState' object has no attribute 'mixed_precision'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "053e55c4-eb33-441c-9f52-7dd714297b8c",
      "metadata": {
        "id": "053e55c4-eb33-441c-9f52-7dd714297b8c",
        "outputId": "852784a2-1633-4a7e-fa91-1adfb49d8988",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are adding a <class 'transformers.integrations.integration_utils.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
            ":DefaultFlowCallback\n",
            "TensorBoardCallback\n",
            "NotebookProgressCallback\n"
          ]
        }
      ],
      "source": [
        "# Add TensorBoard callback\n",
        "trainer.add_callback(TensorBoardCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3b93a8aa-d40e-46a7-90ae-913de5db42eb",
      "metadata": {
        "scrolled": true,
        "id": "3b93a8aa-d40e-46a7-90ae-913de5db42eb",
        "outputId": "ea442292-b3f6-4999-db82-cf70ba7c86d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,062 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 1\n",
            "\\        /    Total batch size = 2 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 29,884,416\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 00:42, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.826800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.692700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.173000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.918300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.849500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.431400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.632900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.716300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.188700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.393700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.385000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.990100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.955200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.791700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.866100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.714600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.697600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.595500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.451900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.461800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.379800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.419200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.368400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.366300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.368800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.358100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.427200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.344100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.223100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.330900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.357200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.367400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.194100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.351400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.332200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.251100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.297300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.318900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.335500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.320900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.256100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.198000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.287800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.169900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.164800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.323400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.148100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.269800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.326000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.340700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.283500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.340500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.300700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.285200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "96bce181-0a39-426b-b2ea-3d03f5c2520d",
      "metadata": {
        "id": "96bce181-0a39-426b-b2ea-3d03f5c2520d",
        "outputId": "de095c4a-c47b-4cd4-8a4d-3d17860cbe9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch.cuda' has no attribute 'empty'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-42b9f9aaba2d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'empty'"
          ]
        }
      ],
      "source": [
        "#torch.cuda.empty()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "54108618-8260-4b16-8ef2-fd80c5deeb16",
      "metadata": {
        "id": "54108618-8260-4b16-8ef2-fd80c5deeb16"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7965239c-9f4c-4cab-836c-330e7de00647",
      "metadata": {
        "scrolled": true,
        "id": "7965239c-9f4c-4cab-836c-330e7de00647",
        "outputId": "def807b8-a07d-48c4-abdb-8b18377c6c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='114' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [34/34 04:51]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f245b31d31e3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3572\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3573\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3574\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3852\u001b[0m                 )\n\u001b[1;32m   3853\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3854\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3855\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3856\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-010fc5accfe4>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpacking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0meval_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-20-74744d830291>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred, split)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdecoded_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdecoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3794\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3795\u001b[0m         \"\"\"\n\u001b[0;32m-> 3796\u001b[0;31m         return [\n\u001b[0m\u001b[1;32m   3797\u001b[0m             self.decode(\n\u001b[1;32m   3798\u001b[0m                 \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3795\u001b[0m         \"\"\"\n\u001b[1;32m   3796\u001b[0m         return [\n\u001b[0;32m-> 3797\u001b[0;31m             self.decode(\n\u001b[0m\u001b[1;32m   3798\u001b[0m                 \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3834\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3836\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3837\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3838\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         clean_up_tokenization_spaces = (\n",
            "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
          ]
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0bc527-374c-452f-87cb-9937cd3372ef",
      "metadata": {
        "id": "aa0bc527-374c-452f-87cb-9937cd3372ef"
      },
      "outputs": [],
      "source": [
        "# val_results = trainer.evaluate(eval_dataset=val_dataset, metric_key_prefix=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d78665b-4749-4929-b0f0-b3138efd64ba",
      "metadata": {
        "id": "0d78665b-4749-4929-b0f0-b3138efd64ba",
        "outputId": "130de9e8-e8dc-4bf5-85e7-59bff543697e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model to https://huggingface.co/VatsalPatel18/phi3-mini-WeatherBot\n"
          ]
        }
      ],
      "source": [
        "model.push_to_hub(\"VatsalPatel18/phi3-mini-WeatherBot\", token=os.getenv(\"HUGGINGFACE_HUB_TOKEN\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6084716f-cc17-4b01-85c6-f5a6de12fc0b",
      "metadata": {
        "id": "6084716f-cc17-4b01-85c6-f5a6de12fc0b",
        "outputId": "16045230-0f3e-4642-e363-937996a9c525"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('../models/trained/tokenizer_config.json',\n",
              " '../models/trained/special_tokens_map.json',\n",
              " '../models/trained/tokenizer.model',\n",
              " '../models/trained/added_tokens.json',\n",
              " '../models/trained/tokenizer.json')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"../models/trained\")\n",
        "tokenizer.save_pretrained(\"../models/trained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d0e288c-05b3-48fd-895c-d877a36b2a33",
      "metadata": {
        "id": "6d0e288c-05b3-48fd-895c-d877a36b2a33"
      },
      "outputs": [],
      "source": [
        "# !pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b59eb1-1148-4d44-ad51-9de1a3823b84",
      "metadata": {
        "scrolled": true,
        "id": "f8b59eb1-1148-4d44-ad51-9de1a3823b84",
        "outputId": "8077594c-53ce-48ac-ac78-61be7b2ff8c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 6.05 out of 14.96 RAM for saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:13<00:00,  2.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model type of mistral auto adds a BOS token.\n",
            "Unsloth: ##### If you're using Ollama or GGUF etc, do not add a BOS in the chat template.\n",
            "Unsloth: Extending ../models/gguf//tokenizer.model with added_tokens.json.\n",
            "Originally tokenizer.model is of size (32000).\n",
            "But we need to extend to sentencepiece vocab size (32011).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n",
            " \"-____-\"     In total, you will have to wait around 26 minutes.\n",
            "\n",
            "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
            "Unsloth: [1] Converting model at ../models/gguf/ into f16 GGUF format.\n",
            "The output location will be ./../models/gguf/-unsloth.F16.gguf\n",
            "This will take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: gguf\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 4096\n",
            "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
            "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 32000\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 32009\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|user|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% elif message['from'] == 'gpt' %}{{'<|assistant|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% else %}{{'<|' + message['from'] + '|>\n",
            "' + message['value'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% endif %}\n",
            "INFO:hf-to-gguf:Exporting model to '../models/gguf/-unsloth.F16.gguf'\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> F16, shape = {3072, 32064}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> F16, shape = {3072, 32064}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {3072}\n",
            "Writing: 100%|██████████| 7.64G/7.64G [00:25<00:00, 295Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to '../models/gguf/-unsloth.F16.gguf'\n",
            "Unsloth: Conversion completed! Output location: ./../models/gguf/-unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
            "main: build = 3131 (148995e5)\n",
            "main: built with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\n",
            "main: quantizing './../models/gguf/-unsloth.F16.gguf' to './../models/gguf/-unsloth.Q4_K_M.gguf' as Q4_K_M using 32 threads\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./../models/gguf/-unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = gguf\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 32009\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                    token_embd.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q4_K .. size =   187.88 MiB ->    52.84 MiB\n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  20/ 291]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 291]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  22/ 291]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  23/ 291]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  24/ 291]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  25/ 291]                 blk.10.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  26/ 291]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  27/ 291]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  28/ 291]                 blk.10.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  29/ 291]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 291]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  31/ 291]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  32/ 291]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  33/ 291]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  34/ 291]                 blk.11.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  35/ 291]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  36/ 291]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  37/ 291]                 blk.11.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  38/ 291]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 291]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  40/ 291]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  41/ 291]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  42/ 291]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  43/ 291]                 blk.12.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  44/ 291]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  45/ 291]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  46/ 291]                 blk.12.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  47/ 291]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 291]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  49/ 291]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  50/ 291]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  51/ 291]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  52/ 291]                 blk.13.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  53/ 291]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  54/ 291]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  55/ 291]                 blk.13.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  56/ 291]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 291]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  58/ 291]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  59/ 291]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  60/ 291]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  61/ 291]                 blk.14.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  62/ 291]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  63/ 291]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  64/ 291]                 blk.14.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  65/ 291]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 291]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  67/ 291]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  68/ 291]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  69/ 291]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  70/ 291]                 blk.15.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  71/ 291]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  72/ 291]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  73/ 291]                 blk.15.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  74/ 291]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 291]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  76/ 291]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  77/ 291]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  78/ 291]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  79/ 291]                 blk.16.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  80/ 291]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  81/ 291]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  82/ 291]                 blk.16.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  83/ 291]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 291]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  85/ 291]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  86/ 291]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  87/ 291]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  88/ 291]                 blk.17.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  89/ 291]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  90/ 291]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  91/ 291]                 blk.17.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[  92/ 291]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 291]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  94/ 291]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  95/ 291]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  96/ 291]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  97/ 291]                 blk.18.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  98/ 291]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  99/ 291]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 100/ 291]                 blk.18.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 101/ 291]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 291]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 103/ 291]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 104/ 291]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 105/ 291]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 106/ 291]                 blk.19.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 107/ 291]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 108/ 291]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 109/ 291]                 blk.19.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 110/ 291]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 291]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 112/ 291]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 113/ 291]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 114/ 291]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 115/ 291]                  blk.2.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 116/ 291]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 117/ 291]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 118/ 291]                  blk.2.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 119/ 291]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 291]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 121/ 291]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 122/ 291]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 123/ 291]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 124/ 291]                 blk.20.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 125/ 291]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 126/ 291]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 127/ 291]                 blk.20.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 128/ 291]                 blk.21.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 129/ 291]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 130/ 291]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 131/ 291]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 132/ 291]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 133/ 291]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 134/ 291]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 135/ 291]                  blk.3.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 136/ 291]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 137/ 291]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 138/ 291]                  blk.3.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 139/ 291]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 140/ 291]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 141/ 291]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 142/ 291]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 143/ 291]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 144/ 291]                  blk.4.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 145/ 291]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 146/ 291]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 147/ 291]                  blk.4.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 148/ 291]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 149/ 291]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 150/ 291]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 151/ 291]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 152/ 291]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 153/ 291]                  blk.5.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 154/ 291]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 155/ 291]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 156/ 291]                  blk.5.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 157/ 291]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 158/ 291]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 159/ 291]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 160/ 291]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 161/ 291]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 162/ 291]                  blk.6.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 163/ 291]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 164/ 291]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 165/ 291]                  blk.6.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 166/ 291]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 167/ 291]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 168/ 291]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 169/ 291]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 170/ 291]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 171/ 291]                  blk.7.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 172/ 291]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 173/ 291]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 174/ 291]                  blk.7.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 175/ 291]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 176/ 291]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 177/ 291]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 178/ 291]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 179/ 291]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 180/ 291]                  blk.8.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 181/ 291]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 182/ 291]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 183/ 291]                  blk.8.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 184/ 291]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 185/ 291]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 186/ 291]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 187/ 291]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 188/ 291]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 189/ 291]                  blk.9.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 190/ 291]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 191/ 291]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 192/ 291]                  blk.9.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 193/ 291]                        output.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q6_K .. size =   187.88 MiB ->    77.06 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 291]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 196/ 291]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 197/ 291]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 198/ 291]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 199/ 291]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 200/ 291]                 blk.21.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 201/ 291]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 202/ 291]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 203/ 291]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 204/ 291]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 205/ 291]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 206/ 291]                 blk.22.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 207/ 291]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 208/ 291]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 209/ 291]                 blk.22.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 210/ 291]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 211/ 291]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 212/ 291]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 213/ 291]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 214/ 291]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 215/ 291]                 blk.23.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 216/ 291]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 217/ 291]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 218/ 291]                 blk.23.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 219/ 291]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 220/ 291]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 221/ 291]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 222/ 291]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 223/ 291]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 224/ 291]                 blk.24.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 225/ 291]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 226/ 291]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 227/ 291]                 blk.24.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 228/ 291]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 229/ 291]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 230/ 291]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 231/ 291]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 232/ 291]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 233/ 291]                 blk.25.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 234/ 291]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 235/ 291]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 236/ 291]                 blk.25.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 237/ 291]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 238/ 291]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 239/ 291]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 240/ 291]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 241/ 291]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 242/ 291]                 blk.26.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 243/ 291]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 244/ 291]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 245/ 291]                 blk.26.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 246/ 291]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 247/ 291]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 248/ 291]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 249/ 291]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 250/ 291]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 251/ 291]                 blk.27.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 252/ 291]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 253/ 291]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 254/ 291]                 blk.27.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 255/ 291]              blk.28.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 256/ 291]               blk.28.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 257/ 291]               blk.28.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 258/ 291]                 blk.28.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 259/ 291]               blk.28.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 260/ 291]                 blk.28.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 261/ 291]            blk.28.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 262/ 291]                 blk.28.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 263/ 291]                 blk.28.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 264/ 291]              blk.29.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 265/ 291]               blk.29.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 266/ 291]               blk.29.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 267/ 291]                 blk.29.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 268/ 291]               blk.29.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 269/ 291]                 blk.29.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 270/ 291]            blk.29.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 271/ 291]                 blk.29.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 272/ 291]                 blk.29.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 273/ 291]              blk.30.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 274/ 291]               blk.30.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 275/ 291]               blk.30.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 276/ 291]                 blk.30.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 277/ 291]               blk.30.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 278/ 291]                 blk.30.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 279/ 291]            blk.30.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 280/ 291]                 blk.30.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 281/ 291]                 blk.30.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 282/ 291]              blk.31.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 283/ 291]               blk.31.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 284/ 291]               blk.31.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 285/ 291]                 blk.31.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 286/ 291]               blk.31.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 287/ 291]                 blk.31.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 288/ 291]            blk.31.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 289/ 291]                 blk.31.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 290/ 291]                 blk.31.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
            "[ 291/ 291]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "llama_model_quantize_internal: model size  =  7288.51 MB\n",
            "llama_model_quantize_internal: quant size  =  2210.78 MB\n",
            "\n",
            "main: quantize time = 37463.92 ms\n",
            "main:    total time = 37463.92 ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model type of mistral auto adds a BOS token.\n",
            "Unsloth: ##### If you're using Ollama or GGUF etc, do not add a BOS in the chat template.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Conversion completed! Output location: ./../models/gguf/-unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained_gguf(\"../models/gguf/\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f81cc3b-d78f-4b0c-885d-22237242d7ec",
      "metadata": {
        "id": "6f81cc3b-d78f-4b0c-885d-22237242d7ec",
        "outputId": "31f06531-6402-4865-ae15-27fab4ee3b3c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_metrics(metrics, title, ylabel):\n",
        "    df = pd.DataFrame(metrics)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for column in metrics:\n",
        "        if column != \"step\":\n",
        "            plt.plot(df[\"step\"], df[column], label=column)\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plotting training metrics\n",
        "plot_metrics(train_metrics, \"Training Metrics\", \"Score\")\n",
        "\n",
        "# Plotting validation metrics\n",
        "plot_metrics(val_metrics, \"Validation Metrics\", \"Score\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6c07711-b771-4768-bd89-e8ba2bfaee2d",
      "metadata": {
        "id": "b6c07711-b771-4768-bd89-e8ba2bfaee2d"
      },
      "outputs": [],
      "source": [
        "# from unsloth import FastLanguageModel\n",
        "\n",
        "# model = FastLanguageModel.from_pretrained(\"path/to/save/model\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"path/to/save/tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f0982d-2ccb-4d4d-89a0-956fa66942f5",
      "metadata": {
        "id": "c1f0982d-2ccb-4d4d-89a0-956fa66942f5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Unsloth)",
      "language": "python",
      "name": "unsloth_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "151fa84982b34bd2943c8710a3323230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b3254e94c074e5cbaf348873dd5eca3",
              "IPY_MODEL_0a47a40627d942189c5bec1c09f18f5f",
              "IPY_MODEL_799c860ac0c947e392446fe239609b74"
            ],
            "layout": "IPY_MODEL_79c1b679c8ac48429256082179cbdaf1"
          }
        },
        "4b3254e94c074e5cbaf348873dd5eca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12e8388295e24026863747a6cb12d0e1",
            "placeholder": "​",
            "style": "IPY_MODEL_4b1abcb8ceef48b2831e2223160d1525",
            "value": "Map: 100%"
          }
        },
        "0a47a40627d942189c5bec1c09f18f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4453dd8db2c247e88eb1a76142b22b74",
            "max": 1062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01e60bb1c87849ee969c519b03e222dd",
            "value": 1062
          }
        },
        "799c860ac0c947e392446fe239609b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a27bdcfc3d294bfc912113651b3dc382",
            "placeholder": "​",
            "style": "IPY_MODEL_90122c6d58ac46e99c53c99df2b52344",
            "value": " 1062/1062 [00:00&lt;00:00, 7196.10 examples/s]"
          }
        },
        "79c1b679c8ac48429256082179cbdaf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12e8388295e24026863747a6cb12d0e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1abcb8ceef48b2831e2223160d1525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4453dd8db2c247e88eb1a76142b22b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01e60bb1c87849ee969c519b03e222dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a27bdcfc3d294bfc912113651b3dc382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90122c6d58ac46e99c53c99df2b52344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a3d97bc82fd40c3b87ff891a57842a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cead9a5820994d81a5d728c7e1c57d6e",
              "IPY_MODEL_546557f8d0d54fcaaa19fba886defa6d",
              "IPY_MODEL_9b3c4f8d1c8d49aa97754044338cc32f"
            ],
            "layout": "IPY_MODEL_8329a931b53d4171a089c0db4e27d43b"
          }
        },
        "cead9a5820994d81a5d728c7e1c57d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3abf2836489c4d03bb5f6b93473c3d9d",
            "placeholder": "​",
            "style": "IPY_MODEL_17e4d85de05b42cebf22a59a973ec6a7",
            "value": "Map: 100%"
          }
        },
        "546557f8d0d54fcaaa19fba886defa6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0912eb86282c43f28db42635cea77a79",
            "max": 266,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66ae625829c84d0fb74cb18c2ce58bd8",
            "value": 266
          }
        },
        "9b3c4f8d1c8d49aa97754044338cc32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e3bef826ac24ddcadd426e506fd1b67",
            "placeholder": "​",
            "style": "IPY_MODEL_6610b30247174f00b2acad688dd9655b",
            "value": " 266/266 [00:00&lt;00:00, 4375.29 examples/s]"
          }
        },
        "8329a931b53d4171a089c0db4e27d43b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3abf2836489c4d03bb5f6b93473c3d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e4d85de05b42cebf22a59a973ec6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0912eb86282c43f28db42635cea77a79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66ae625829c84d0fb74cb18c2ce58bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e3bef826ac24ddcadd426e506fd1b67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6610b30247174f00b2acad688dd9655b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}