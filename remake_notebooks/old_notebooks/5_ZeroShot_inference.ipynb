{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df07ace2-3b89-4fea-b69f-8789cd0d4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from transformers.agents import Tool\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e27b11d-5101-49d7-9847-62ac8f93cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherTool(Tool):\n",
    "    name = \"weather_api\"\n",
    "    description = \"Retrieves weather information for a specific city.\"\n",
    "    inputs = {\n",
    "        \"city\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The name of the city to get weather information for.\"\n",
    "        },\n",
    "        \"forecast_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Either 'current' for current weather or 'forecast' for future weather.\"\n",
    "        }\n",
    "    }\n",
    "    output_type = \"text\"\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        super().__init__()\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def __call__(self, city: str, forecast_type: str = \"current\") -> str:\n",
    "        base_url = \"http://api.openweathermap.org/data/2.5\"\n",
    "        \n",
    "        if forecast_type == \"current\":\n",
    "            endpoint = f\"{base_url}/weather\"\n",
    "        else:\n",
    "            endpoint = f\"{base_url}/forecast\"\n",
    "        \n",
    "        params = {\n",
    "            \"q\": city,\n",
    "            \"appid\": self.api_key,\n",
    "            \"units\": \"metric\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(endpoint, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if forecast_type == \"current\":\n",
    "                temp = data['main']['temp']\n",
    "                description = data['weather'][0]['description']\n",
    "                return f\"The current weather in {city} is {description} with a temperature of {temp}°C.\"\n",
    "            else:\n",
    "                forecast = data['list'][0]  # Get the first forecast entry\n",
    "                temp = forecast['main']['temp']\n",
    "                description = forecast['weather'][0]['description']\n",
    "                date_time = forecast['dt_txt']\n",
    "                return f\"The forecast for {city} on {date_time} is {description} with a temperature of {temp}°C.\"\n",
    "        else:\n",
    "            return f\"Error: Unable to fetch weather data for {city}. Status code: {response.status_code}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb73086b-60b4-48df-bbe1-3bc63551b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Tool, load_tool, ReactCodeAgent, HfEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f50c64-163d-47ff-8220-5abddac79377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mWhat's the weather like in London today?\u001b[0m\n",
      "\u001b[31;20mError in generating llm output: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120)..\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 273, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions (Request ID: BuxEwERz_JFR8anpyZAPX)\n",
      "\n",
      "Model unsloth/Phi-3-mini-4k-instruct is currently loading\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 798, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_code>\", \"Observation:\"])\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 85, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 706, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 283, in post\n",
      "    raise InferenceTimeoutError(\n",
      "huggingface_hub.errors.InferenceTimeoutError: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 623, in run\n",
      "    final_answer = self.step()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 800, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120)..\n",
      "\u001b[31;20mError in generating llm output: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120)..\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 273, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions (Request ID: HpIDUvBZsB7Tt3fxJtcQT)\n",
      "\n",
      "Model unsloth/Phi-3-mini-4k-instruct is currently loading\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 798, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_code>\", \"Observation:\"])\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 85, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 706, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 283, in post\n",
      "    raise InferenceTimeoutError(\n",
      "huggingface_hub.errors.InferenceTimeoutError: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 623, in run\n",
      "    final_answer = self.step()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 800, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120)..\n",
      "\u001b[31;20mError in generating llm output: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120)..\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 273, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions (Request ID: 8kL_04i57Sv2F4KltXZ-W)\n",
      "\n",
      "Model unsloth/Phi-3-mini-4k-instruct is currently loading\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 798, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_code>\", \"Observation:\"])\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 85, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 706, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 283, in post\n",
      "    raise InferenceTimeoutError(\n",
      "huggingface_hub.errors.InferenceTimeoutError: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120).\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 623, in run\n",
      "    final_answer = self.step()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 800, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: Model not loaded on the server: https://api-inference.huggingface.co/models/unsloth/Phi-3-mini-4k-instruct/v1/chat/completions. Please retry with a higher timeout (current: 120)..\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;7mcurrent_weather\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mweather_api\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7m{\u001b[39m\n",
      "\u001b[38;5;7m    \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mcity\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m:\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7m{\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mtype\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m:\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mstring\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mdescription\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m:\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mThe city to get weather information for.\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m}\u001b[39m\u001b[38;5;7m,\u001b[39m\n",
      "\u001b[38;5;7m    \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mforecast_type\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m:\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7m{\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mtype\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m:\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mstring\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m,\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mdescription\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m:\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mEither \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mcurrent\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144m for current weather or \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mforecast\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144m for future weather.\u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;7m}\u001b[39m\n",
      "\u001b[38;5;7m}\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mcurrent_weather\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mThe current weather in {'city': {'type': 'string', 'description': 'The city to get weather information for.'}, 'forecast_type': {'type': 'string', 'description': \"Either 'current' for current weather or 'forecast' for future weather.\"}} is clear sky with a temperature of 1.34°C.\n",
      "\u001b[0m\n",
      "\u001b[31;20mError in generating llm output: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\"), '(Request ID: f2fb3e0d-bdda-4415-b53f-13220a72651f)').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 537, in _make_request\n",
      "    response = conn.getresponse()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/urllib3/connection.py\", line 466, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/ssl.py\", line 1307, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/ssl.py\", line 1163, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 847, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/urllib3/util/retry.py\", line 470, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 539, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 370, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 798, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_code>\", \"Observation:\"])\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 85, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 706, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 259, in post\n",
      "    response = get_session().post(\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 66, in send\n",
      "    return super().send(request, *args, **kwargs)\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/requests/adapters.py\", line 713, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\"), '(Request ID: f2fb3e0d-bdda-4415-b53f-13220a72651f)')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 623, in run\n",
      "    final_answer = self.step()\n",
      "  File \"/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 800, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\"), '(Request ID: f2fb3e0d-bdda-4415-b53f-13220a72651f)').\n",
      "\u001b[33;1m==== Agent is executing the code below:\u001b[0m\n",
      "\u001b[0m\u001b[38;5;60;03m# Define the city name and forecast type\u001b[39;00m\n",
      "\u001b[38;5;7mcity_name\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mLondon\u001b[39m\u001b[38;5;144m\"\u001b[39m\n",
      "\u001b[38;5;7mforecast_type\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;144m\"\u001b[39m\u001b[38;5;144mcurrent\u001b[39m\u001b[38;5;144m\"\u001b[39m\n",
      "\n",
      "\u001b[38;5;60;03m# Construct the API request\u001b[39;00m\n",
      "\u001b[38;5;7mweather_data\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;109;01m=\u001b[39;00m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mweather_api\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7m{\u001b[39m\n",
      "\u001b[38;5;7m    \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mcity\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m:\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mcity_name\u001b[39m\u001b[38;5;7m,\u001b[39m\n",
      "\u001b[38;5;7m    \u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;144mforecast_type\u001b[39m\u001b[38;5;144m'\u001b[39m\u001b[38;5;7m:\u001b[39m\u001b[38;5;7m \u001b[39m\u001b[38;5;7mforecast_type\u001b[39m\n",
      "\u001b[38;5;7m}\u001b[39m\u001b[38;5;7m)\u001b[39m\n",
      "\n",
      "\u001b[38;5;109mprint\u001b[39m\u001b[38;5;7m(\u001b[39m\u001b[38;5;7mweather_data\u001b[39m\u001b[38;5;7m)\u001b[39m\u001b[0m\n",
      "\u001b[33;1m====\u001b[0m\n",
      "\u001b[33;1mPrint outputs:\u001b[0m\n",
      "\u001b[32;20mThe current weather in {'city': 'London', 'forecast_type': 'current'} is clear sky with a temperature of 1.14°C.\n",
      "\u001b[0m\n",
      "\u001b[31;20mReached max iterations.\u001b[0m\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in London is a clear sky with a temperature of approximately 1.14°C (33.85°F). Considering these temperatures, it implies that it is quite cold and possibly the time is during the early morning or late night. It would be advisable to dress warmly if you plan to be outdoors.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WeatherTool\n",
    "api_key = 'c6dfc4d92a8f972d237ef696ec87b37a'\n",
    "weather_tool = WeatherTool(api_key)\n",
    "\n",
    "# Initialize the LLM engine\n",
    "llm_engine = HfEngine(\"unsloth/Phi-3-mini-4k-instruct\") \n",
    "\n",
    "# Initialize the agent with the weather tool\n",
    "agent = ReactCodeAgent(tools=[weather_tool], llm_engine=llm_engine)\n",
    "\n",
    "# Run it!\n",
    "result = agent.run(\n",
    "    \"What's the weather like in London today?\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137bfbf1-7650-4d4e-a567-84a4bd50b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b4bb0-7fd8-4acd-b050-d0f6f1f0fb96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Unsloth)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
