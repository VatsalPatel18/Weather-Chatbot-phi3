{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0c3d45b1-370a-4187-814a-0eaad9804f95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from ../models/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens cache size = 323\n",
      "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2281.66 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 1024\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   168.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "# Import the Llama class of llama-cpp-python and the LlamaCppPythonProvider of llama-cpp-agent\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp_agent.providers import LlamaCppPythonProvider\n",
    "\n",
    "# Create an instance of the Llama class and load the model\n",
    "llama_model = Llama(r\"../models/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf\", n_batch=1024, n_threads=10, n_gpu_layers=40,n_ctx=2048)\n",
    "\n",
    "# Create the provider by passing the Llama class instance to the LlamaCppPythonProvider class\n",
    "provider = LlamaCppPythonProvider(llama_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38cc4b9c-7e1b-4563-84f1-4b4c0b7a82d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp_agent import LlamaCppAgent\n",
    "\n",
    "agent = LlamaCppAgent(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e675d7d-8085-4f07-a37c-5b98cfa1ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp_agent import LlamaCppAgent\n",
    "from llama_cpp_agent import MessagesFormatterType\n",
    "\n",
    "agent = LlamaCppAgent(provider,\n",
    "                      system_prompt=\"You are a Weather Chatbot.\",\n",
    "                      predefined_messages_formatter_type=MessagesFormatterType.CHATML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22beea11-4cbc-4222-a31a-8772f0729250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1277.82 ms\n",
      "llama_print_timings:      sample time =      11.35 ms /    75 runs   (    0.15 ms per token,  6610.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1277.51 ms /    60 tokens (   21.29 ms per token,    46.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4775.29 ms /    74 runs   (   64.53 ms per token,    15.50 tokens per second)\n",
      "llama_print_timings:       total time =    6098.85 ms /   134 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Hello! As your weather-informed assistant, I don't have feelings, but I'm here and ready to help you with the latest weather updates. How may I assist you today? If you are asking about my 'wellbeing', imagine it as being fully operational and equipped for providing accurate weather information!\n"
     ]
    }
   ],
   "source": [
    "agent_output = agent.get_chat_response(\"Hello, How are you doing Today ?\")\n",
    "print(f\"Agent: {agent_output.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4bdf9d1-c2ca-4b18-90eb-02f33af75179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp_agent.chat_history import BasicChatHistory, BasicChatMessageStore, BasicChatHistoryStrategy\n",
    "\n",
    "\n",
    "chat_history_store = BasicChatMessageStore()\n",
    "\n",
    "chat_history = BasicChatHistory(message_store=chat_history_store, chat_history_strategy=BasicChatHistoryStrategy.last_k_tokens, k=7000, llm_provider=provider)\n",
    "\n",
    "agent = LlamaCppAgent(provider,\n",
    "                      system_prompt=\"You are a Weather Chatbot\",\n",
    "                      chat_history=chat_history,\n",
    "                      predefined_messages_formatter_type=MessagesFormatterType.CHATML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "470fb02e-021f-4a2d-aa57-0faef103ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp_agent.llm_output_settings import LlmStructuredOutputSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "542cff98-127f-4562-9575-7679603b32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7595c4bd-eb68-44d5-ac36-6a80b275760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_a_to_the_power_b(a: Union[int, float], b: Union[int, float]):\n",
    "    \"\"\"\n",
    "    Calculates a to the power of b\n",
    "\n",
    "    Args:\n",
    "        a: number\n",
    "        b: exponent\n",
    "\n",
    "    \"\"\"\n",
    "    return f\"Result: {math.pow(a, b)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38a4b353-c341-4ea3-95a7-98d9589caba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_settings = LlmStructuredOutputSettings.from_functions([calculate_a_to_the_power_b], allow_parallel_function_calling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99b628f5-6dbf-4e4f-bc3a-f894aad32d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bcf1c5c4-434b-4813-8b0c-c49ce3e22894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= root_1 [[] ws function root_5 [<U+000A>] []] \n",
      "root_1 ::= [ ] | [<U+000A>] \n",
      "ws ::= ws_19 \n",
      "function ::= function_6 [{] ws [\"] [f] [u] [n] [c] [t] [i] [o] [n] [\"] [:] [ ] ws grammar-models \n",
      "root_4 ::= [,] ws function \n",
      "root_5 ::= root_4 root_5 | \n",
      "function_6 ::= [ ] | [<U+000A>] \n",
      "grammar-models ::= calculate-a-to-the-power-b-grammar-model \n",
      "calculate-a-to-the-power-b-grammar-model ::= [\"] [c] [a] [l] [c] [u] [l] [a] [t] [e] [_] [a] [_] [t] [o] [_] [t] [h] [e] [_] [p] [o] [w] [e] [r] [_] [b] [\"] [,] ws [\"] [a] [r] [g] [u] [m] [e] [n] [t] [s] [\"] [:] [ ] calculate-a-to-the-power-b \n",
      "calculate-a-to-the-power-b ::= [{] ws [\"] [a] [\"] [:] [ ] calculate-a-to-the-power-b-a-union [,] ws [\"] [b] [\"] [:] [ ] calculate-a-to-the-power-b-b-union ws [}] ws [}] \n",
      "calculate-a-to-the-power-b-a-union ::= number \n",
      "calculate-a-to-the-power-b-b-union ::= number \n",
      "number ::= number_21 number_22 number_29 \n",
      "boolean ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "null ::= [n] [u] [l] [l] \n",
      "string ::= [\"] string_18 [\"] \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "ws_19 ::= ws_20 \n",
      "ws_20 ::= [ <U+0009><U+000A>] ws_20 | [ <U+0009><U+000A>] \n",
      "number_21 ::= [-] | \n",
      "number_22 ::= number_23 | number_24 [.] number_25 \n",
      "number_23 ::= [0-9] number_23 | [0-9] \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= [0-9] number_25 | [0-9] \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Read and follow the instructions below:\n",
      "\n",
      "<system_instructions>\n",
      "You are an advanced AI, tasked to assist the user by calling functions in JSON format.\n",
      "</system_instructions>\n",
      "\n",
      "\n",
      "You can call functions to help you with your tasks and user queries. The available functions are:\n",
      "\n",
      "<function_list>\n",
      "Function: calculate_a_to_the_power_b\n",
      "  Description: Calculates a to the power of b\n",
      "  Parameters:\n",
      "    a (int or float): number\n",
      "    b (int or float): exponent\n",
      "</function_list>\n",
      "\n",
      "To call a function, respond with a JSON object (to call one function) or a list of JSON objects (to call multiple functions), with each object containing these fields:\n",
      "\n",
      "- \"function\": Put the name of the function to call here. \n",
      "- \"arguments\": Put the arguments to pass to the function here.\n",
      "\n",
      "The result of each function call will be returned to you before you need to respond again.<|im_end|>\n",
      "<|im_start|>user\n",
      "Calculate a to the power of b: a = 2, b = 4<|im_end|>\n",
      "<|im_start|>assistant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1277.82 ms\n",
      "llama_print_timings:      sample time =     127.26 ms /    56 runs   (    2.27 ms per token,   440.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5772.81 ms /   269 tokens (   21.46 ms per token,    46.60 tokens per second)\n",
      "llama_print_timings:        eval time =    3743.05 ms /    55 runs   (   68.06 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =    9707.40 ms /   324 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[\n",
      "    {\n",
      "        \"function\":  \"calculate_a_to_the_power_b\",\n",
      "        \"arguments\": {\n",
      "            \"a\": 2,\n",
      "            \"b\": 4\n",
      "        }\n",
      "    }\n",
      "]\n",
      "[{'function': 'calculate_a_to_the_power_b', 'arguments': {'a': 2, 'b': 4}, 'return_value': 'Result: 16.0'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a LlamaCppAgent instance as before, including a system message with information about the tools available for the LLM agent.\n",
    "llama_cpp_agent = LlamaCppAgent(\n",
    "    provider,\n",
    "    debug_output=True,\n",
    "    system_prompt=f\"You are an advanced AI, tasked to assist the user by calling functions in JSON format.\",\n",
    "    predefined_messages_formatter_type=MessagesFormatterType.CHATML,\n",
    ")\n",
    "\n",
    "# Define some user input\n",
    "user_input = \"Calculate a to the power of b: a = 2, b = 4\"\n",
    "\n",
    "# Pass the user input together with output settings to `get_chat_response` method.\n",
    "# This will print the result of the function the LLM will call, it is a list of dictionaries containing the result.\n",
    "print(\n",
    "    llama_cpp_agent.get_chat_response(\n",
    "        user_input, structured_output_settings=output_settings\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b3478c37-f098-4807-a410-def88119a8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Read and follow the instructions below:\n",
      "\n",
      "<system_instructions>\n",
      "You are an advanced AI, tasked to assist the user by calling functions in JSON format.\n",
      "</system_instructions>\n",
      "\n",
      "\n",
      "You can call functions to help you with your tasks and user queries. The available functions are:\n",
      "\n",
      "<function_list>\n",
      "Function: calculate_a_to_the_power_b\n",
      "  Description: Calculates a to the power of b\n",
      "  Parameters:\n",
      "    a (int or float): number\n",
      "    b (int or float): exponent\n",
      "</function_list>\n",
      "\n",
      "To call a function, respond with a JSON object (to call one function) or a list of JSON objects (to call multiple functions), with each object containing these fields:\n",
      "\n",
      "- \"function\": Put the name of the function to call here. \n",
      "- \"arguments\": Put the arguments to pass to the function here.\n",
      "\n",
      "The result of each function call will be returned to you before you need to respond again.<|im_end|>\n",
      "<|im_start|>user\n",
      "Calculate a to the power of b: a = 2, b = 4<|im_end|>\n",
      "<|im_start|>assistant\n",
      "[\n",
      "    {\n",
      "        \"function\":  \"calculate_a_to_the_power_b\",\n",
      "        \"arguments\": {\n",
      "            \"a\": 2,\n",
      "            \"b\": 4\n",
      "        }\n",
      "    }\n",
      "]<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me a joke ?<|im_end|>\n",
      "<|im_start|>assistant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1277.82 ms\n",
      "llama_print_timings:      sample time =     163.62 ms /    55 runs   (    2.97 ms per token,   336.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1406.95 ms /    66 tokens (   21.32 ms per token,    46.91 tokens per second)\n",
      "llama_print_timings:        eval time =    4013.30 ms /    54 runs   (   74.32 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time =    5655.78 ms /   120 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[\n",
      "    {\n",
      "        \"function\":  \"calculate_a_to_the_power_b\",\n",
      "        \"arguments\": {\n",
      "            \"a\": 2,\n",
      "            \"b\": 4\n",
      "        }\n",
      "    }\n",
      "]\n",
      "[{'function': 'calculate_a_to_the_power_b', 'arguments': {'a': 2, 'b': 4}, 'return_value': 'Result: 16.0'}]\n"
     ]
    }
   ],
   "source": [
    "# Define some user input\n",
    "user_input = \"Can you tell me a joke ?\"\n",
    "\n",
    "print(\n",
    "    llama_cpp_agent.get_chat_response(\n",
    "        user_input, structured_output_settings=output_settings\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "249c4e28-b6b3-4f2a-8424-0b63cba3ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import Union\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from llama_cpp_agent import FunctionCallingAgent\n",
    "from llama_cpp_agent import MessagesFormatterType\n",
    "from llama_cpp_agent import LlamaCppFunctionTool\n",
    "from llama_cpp_agent.providers import TGIServerProvider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5dd87d2d-c36b-4519-aacf-451e1b03c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple calculator tool for the agent that can add, subtract, multiply, and divide.\n",
    "class MathOperation(Enum):\n",
    "    ADD = \"add\"\n",
    "    SUBTRACT = \"subtract\"\n",
    "    MULTIPLY = \"multiply\"\n",
    "    DIVIDE = \"divide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "df40b03e-02e7-4f38-b454-ffedaf68bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calculator(BaseModel):\n",
    "    \"\"\"\n",
    "    Perform a math operation on two numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    number_one: Union[int, float] = Field(\n",
    "        ...,\n",
    "        description=\"First number.\")\n",
    "    number_two: Union[int, float] = Field(\n",
    "        ...,\n",
    "        description=\"Second number.\")\n",
    "    operation: MathOperation = Field(..., description=\"Math operation to perform.\")\n",
    "\n",
    "    def run(self):\n",
    "        if self.operation == MathOperation.ADD:\n",
    "            return self.number_one + self.number_two\n",
    "        elif self.operation == MathOperation.SUBTRACT:\n",
    "            return self.number_one - self.number_two\n",
    "        elif self.operation == MathOperation.MULTIPLY:\n",
    "            return self.number_one * self.number_two\n",
    "        elif self.operation == MathOperation.DIVIDE:\n",
    "            return self.number_one / self.number_two\n",
    "        else:\n",
    "            raise ValueError(\"Unknown operation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d95d5041-89a7-4992-8a1f-22160f0cef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= root_1 [[] ws function root_5 [<U+000A>] []] \n",
      "root_1 ::= [ ] | [<U+000A>] \n",
      "ws ::= ws_22 \n",
      "function ::= function_6 [{] ws [\"] [t] [h] [o] [u] [g] [h] [t] [s] [_] [a] [n] [d] [_] [r] [e] [a] [s] [o] [n] [i] [n] [g] [\"] [:] ws string [,] ws [\"] [f] [u] [n] [c] [t] [i] [o] [n] [\"] [:] ws grammar-models \n",
      "root_4 ::= [,] ws function \n",
      "root_5 ::= root_4 root_5 | \n",
      "function_6 ::= [ ] | [<U+000A>] \n",
      "string ::= [\"] string_21 [\"] \n",
      "grammar-models ::= calculator-grammar-model | send-message-grammar-model \n",
      "calculator-grammar-model ::= [\"] [C] [a] [l] [c] [u] [l] [a] [t] [o] [r] [\"] [,] ws [\"] [a] [r] [g] [u] [m] [e] [n] [t] [s] [\"] [:] [ ] calculator \n",
      "send-message-grammar-model ::= [\"] [s] [e] [n] [d] [_] [m] [e] [s] [s] [a] [g] [e] [\"] [,] ws [\"] [a] [r] [g] [u] [m] [e] [n] [t] [s] [\"] [:] [ ] send-message \n",
      "calculator ::= [{] ws [\"] [n] [u] [m] [b] [e] [r] [_] [o] [n] [e] [\"] [:] [ ] calculator-number-one-union [,] ws [\"] [n] [u] [m] [b] [e] [r] [_] [t] [w] [o] [\"] [:] [ ] calculator-number-two-union [,] ws [\"] [o] [p] [e] [r] [a] [t] [i] [o] [n] [\"] [:] [ ] calculator-operation ws [}] ws [}] \n",
      "send-message ::= [{] ws [\"] [c] [o] [n] [t] [e] [n] [t] [\"] [:] [ ] string ws [}] ws [}] \n",
      "calculator-number-one-union ::= number \n",
      "calculator-number-two-union ::= number \n",
      "calculator-operation ::= [\"] [a] [d] [d] [\"] | [\"] [s] [u] [b] [t] [r] [a] [c] [t] [\"] | [\"] [m] [u] [l] [t] [i] [p] [l] [y] [\"] | [\"] [d] [i] [v] [i] [d] [e] [\"] \n",
      "number ::= number_24 number_25 number_32 \n",
      "boolean ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "null ::= [n] [u] [l] [l] \n",
      "string_19 ::= [^\"\\] | [\\] string_20 \n",
      "string_20 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_21 ::= string_19 string_21 | \n",
      "ws_22 ::= ws_23 \n",
      "ws_23 ::= [ <U+0009><U+000A>] ws_23 | [ <U+0009><U+000A>] \n",
      "number_24 ::= [-] | \n",
      "number_25 ::= number_26 | number_27 [.] number_28 \n",
      "number_26 ::= [0-9] number_26 | [0-9] \n",
      "number_27 ::= [0-9] number_27 | [0-9] \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= [eE] number_30 number_31 \n",
      "number_30 ::= [-+] | \n",
      "number_31 ::= [0-9] number_31 | [0-9] \n",
      "number_32 ::= number_29 | \n",
      "\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1277.82 ms\n",
      "llama_print_timings:      sample time =    1533.10 ms /   533 runs   (    2.88 ms per token,   347.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11609.01 ms /   471 tokens (   24.65 ms per token,    40.57 tokens per second)\n",
      "llama_print_timings:        eval time =   41986.79 ms /   532 runs   (   78.92 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:       total time =   55934.24 ms /  1003 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1277.82 ms\n",
      "llama_print_timings:      sample time =    1481.50 ms /   526 runs   (    2.82 ms per token,   355.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5247.46 ms /   156 tokens (   33.64 ms per token,    29.73 tokens per second)\n",
      "llama_print_timings:        eval time =   44671.70 ms /   525 runs   (   85.09 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =   52215.68 ms /   681 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1277.82 ms\n",
      "llama_print_timings:      sample time =     649.39 ms /   179 runs   (    3.63 ms per token,   275.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4187.52 ms /   156 tokens (   26.84 ms per token,    37.25 tokens per second)\n",
      "llama_print_timings:        eval time =   15463.87 ms /   178 runs   (   86.88 ms per token,    11.51 tokens per second)\n",
      "llama_print_timings:       total time =   20568.28 ms /   334 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON: Unterminated string starting at: line 6 column 24 (char 276)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error: Invalid function call response.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callback for receiving messages for the user.\n",
    "def send_message_to_user_callback(message: str):\n",
    "    print(message)\n",
    "\n",
    "\n",
    "# Create a list of function call tools.\n",
    "function_tools = [LlamaCppFunctionTool(Calculator)]\n",
    "\n",
    "# Create the function calling agent. We are passing the provider, the tool list, send message to user callback and the chat message formatter. Also, we allow parallel function calling.\n",
    "function_call_agent = FunctionCallingAgent(\n",
    "    provider,\n",
    "    llama_cpp_function_tools=function_tools,\n",
    "    allow_parallel_function_calling=True,\n",
    "    send_message_to_user_callback=send_message_to_user_callback,\n",
    "    messages_formatter_type=MessagesFormatterType.CHATML)\n",
    "\n",
    "# Define the user input.\n",
    "user_input = \"Solve the following calculations: 42 * 42, 24 * 24, 5 * 5, 422 * 420, 753 * 321, 789 * 654?\"\n",
    "function_call_agent.generate_response(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1d69c66-d442-4387-83b3-260848e852c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from enum import Enum\n",
    "from typing import Union, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from llama_cpp_agent import LlamaCppFunctionTool\n",
    "from llama_cpp_agent import FunctionCallingAgent\n",
    "from llama_cpp_agent import MessagesFormatterType\n",
    "from llama_cpp_agent.providers import TGIServerProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97125269-e819-48ee-a2d6-9ccf4d317552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tool for the agent, to get the current date and time in a specific format.\n",
    "def get_current_datetime(output_format: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Get the current date and time in the given format.\n",
    "\n",
    "    Args:\n",
    "         output_format: formatting string for the date and time, defaults to '%Y-%m-%d %H:%M:%S'\n",
    "    \"\"\"\n",
    "    if output_format is None:\n",
    "        output_format = '%Y-%m-%d %H:%M:%S'\n",
    "    return datetime.datetime.now().strftime(output_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "77d74e7e-13ea-4339-bd6e-3d598bc8e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathOperation(Enum):\n",
    "    ADD = \"add\"\n",
    "    SUBTRACT = \"subtract\"\n",
    "    MULTIPLY = \"multiply\"\n",
    "    DIVIDE = \"divide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ee5044b8-247b-421f-81e1-29fc47d20da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple pydantic calculator tool for the agent that can add, subtract, multiply, and divide. Docstring and description of fields will be used in system prompt.\n",
    "class calculator(BaseModel):\n",
    "    \"\"\"\n",
    "    Perform a math operation on two numbers.\n",
    "    \"\"\"\n",
    "    number_one: Union[int, float] = Field(..., description=\"First number.\")\n",
    "    operation: MathOperation = Field(..., description=\"Math operation to perform.\")\n",
    "    number_two: Union[int, float] = Field(..., description=\"Second number.\")\n",
    "\n",
    "    def run(self):\n",
    "        if self.operation == MathOperation.ADD:\n",
    "            return self.number_one + self.number_two\n",
    "        elif self.operation == MathOperation.SUBTRACT:\n",
    "            return self.number_one - self.number_two\n",
    "        elif self.operation == MathOperation.MULTIPLY:\n",
    "            return self.number_one * self.number_two\n",
    "        elif self.operation == MathOperation.DIVIDE:\n",
    "            return self.number_one / self.number_two\n",
    "        else:\n",
    "            raise ValueError(\"Unknown operation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "439b8e6d-93e0-4790-af79-0f1f1ea08c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message_to_user_callback(message: str):\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "66e28c89-a6af-49eb-bdfb-f45ebafeefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create the calculator tool.\n",
    "calculator_function_tool = LlamaCppFunctionTool(calculator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "54e04bfe-821f-44e7-88d0-842e1e915557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we create the current datetime tool.\n",
    "current_datetime_function_tool = LlamaCppFunctionTool(get_current_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "81288046-2e35-43bf-9764-59ebd945700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The from_openai_tool function of the LlamaCppFunctionTool class converts an OpenAI tool schema and a callable function into a LlamaCppFunctionTool\n",
    "get_weather_function_tool = LlamaCppFunctionTool.from_openai_tool(open_ai_tool_spec, get_current_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3397368f-664b-48df-b6a9-93ae96e99987",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call_agent = FunctionCallingAgent(\n",
    "    provider,\n",
    "    llama_cpp_function_tools=[calculator_function_tool, current_datetime_function_tool, get_weather_function_tool],\n",
    "    send_message_to_user_callback=send_message_to_user_callback,\n",
    "    allow_parallel_function_calling=True,\n",
    "    messages_formatter_type=MessagesFormatterType.CHATML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ff880a1-6498-42df-a24d-5ae2f97ce31f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (2093) exceed context window of 2048",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# user_input = '''Get the date and time in '%d-%m-%Y %H:%M' format. Get the current weather in celsius in London. Solve the following calculations: 42 * 42, 7 * 26, 4 + 6  and 96/8.'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGet the current weather in celsius in London?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mfunction_call_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp_agent/function_calling_agent.py:234\u001b[0m, in \u001b[0;36mFunctionCallingAgent.generate_response\u001b[0;34m(self, message, llm_sampling_settings, structured_output_settings)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m structured_output_settings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     structured_output_settings\u001b[38;5;241m.\u001b[39madd_thoughts_and_reasoning_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintern_get_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_sampling_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_sampling_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructured_output_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_output_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp_agent/function_calling_agent.py:279\u001b[0m, in \u001b[0;36mFunctionCallingAgent.intern_get_response\u001b[0;34m(self, llm_sampling_settings, structured_output_settings)\u001b[0m\n\u001b[1;32m    277\u001b[0m     without_grammar_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwithout_grammar_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_cpp_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreaming_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreaming_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstructured_output_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_output_settings\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstructured_output_settings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstructured_output_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_sampling_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_sampling_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m without_grammar_mode:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp_agent/llm_agent.py:296\u001b[0m, in \u001b[0;36mLlamaCppAgent.get_chat_response\u001b[0;34m(self, message, role, prompt_suffix, chat_history, system_prompt, system_prompt_modules, add_message_to_chat_history, add_response_to_chat_history, structured_output_settings, llm_sampling_settings, streaming_callback, returns_streaming_generator, print_output)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm_sampling_settings\u001b[38;5;241m.\u001b[39mget_additional_stop_sequences() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     llm_sampling_settings\u001b[38;5;241m.\u001b[39madd_additional_stop_sequences(\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages_formatter\u001b[38;5;241m.\u001b[39mdefault_stop_sequences\n\u001b[1;32m    294\u001b[0m     )\n\u001b[0;32m--> 296\u001b[0m completion, response_role \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response_role_and_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_prompt_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_message_to_chat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_message_to_chat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstructured_output_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructured_output_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_sampling_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_sampling_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream_results\u001b[39m():\n\u001b[1;32m    309\u001b[0m     full_response_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp_agent/llm_agent.py:648\u001b[0m, in \u001b[0;36mLlamaCppAgent.get_response_role_and_completion\u001b[0;34m(self, message, chat_history, system_prompt, system_prompt_modules, add_message_to_chat_history, role, prompt_suffix, llm_sampling_settings, structured_output_settings)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug_output:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prompt, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_provider_identifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mLlmProviderId\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstructured_output_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_sampling_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    654\u001b[0m     response_role,\n\u001b[1;32m    655\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp_agent/providers/llama_cpp_python.py:163\u001b[0m, in \u001b[0;36mLlamaCppPythonProvider.create_completion\u001b[0;34m(self, prompt, structured_output_settings, settings, bos_token)\u001b[0m\n\u001b[1;32m    157\u001b[0m settings_dictionary \u001b[38;5;241m=\u001b[39m deepcopy(settings\u001b[38;5;241m.\u001b[39mas_dict())\n\u001b[1;32m    159\u001b[0m settings_dictionary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m settings_dictionary\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_stop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings_dictionary\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp/llama.py:1607\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1605\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1607\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp/llama.py:1085\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (2093) exceed context window of 2048"
     ]
    }
   ],
   "source": [
    "# user_input = '''Get the date and time in '%d-%m-%Y %H:%M' format. Get the current weather in celsius in London. Solve the following calculations: 42 * 42, 7 * 26, 4 + 6  and 96/8.'''\n",
    "user_input = 'Get the current weather in celsius in London?'\n",
    "function_call_agent.generate_response(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e09b61-164c-4659-90ac-28b1e9ce31e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Unsloth)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
