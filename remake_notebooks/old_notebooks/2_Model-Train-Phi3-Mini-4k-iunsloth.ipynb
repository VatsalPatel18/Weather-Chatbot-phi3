{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9097f8a2-6394-4d53-bd2f-4c6ede9717b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-rvx7roum/unsloth_1da7ff77a5234ed88f8b268c0e57d9de\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-rvx7roum/unsloth_1da7ff77a5234ed88f8b268c0e57d9de\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 8d9bd0ea8bf662618ba96fe7fe3478c5b81d0dff\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tyro in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.4)\n",
      "Requirement already satisfied: transformers>=4.38.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.41.2)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n",
      "Requirement already satisfied: sentencepiece in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.65.0)\n",
      "Requirement already satisfied: psutil in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
      "Requirement already satisfied: numpy in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: filelock in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.2)\n",
      "Requirement already satisfied: packaging in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.9.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.3.5)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Requirement already satisfied: xformers in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (0.0.26.post1)\n",
      "Requirement already satisfied: trl<0.9.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (0.8.6)\n",
      "Requirement already satisfied: peft in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (0.11.1)\n",
      "Requirement already satisfied: accelerate in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (0.30.1)\n",
      "Requirement already satisfied: bitsandbytes in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (0.43.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fae2edfa-163e-439f-a906-775fadb3d2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf8668a6-2b3e-4cbb-ae94-b2cccf9fda7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.64.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/vatsal-patel/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a31650b-4a44-463d-b97d-8abab6357315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vatsal-patel/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "import requests\n",
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import random\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.integrations import TensorBoardCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d318eca-630a-4467-b81f-64b1980eaff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "max_seq_length = 512 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb08d31-4fe6-431c-9679-b0a419847d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0de2c83-6056-4a2f-bab9-111963a2d59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU. Max memory: 3.811 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e32e83-beca-470f-a30b-845ac67b40a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7681fea0-f92a-4b7b-b389-95edf91acb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "dataset_path = 'data/weather_chatbot_dataset.json'\n",
    "with open(dataset_path, 'r') as f:\n",
    "    weather_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd84fe32-7964-4698-8eef-935d10ad0437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'What is the current weather in Frankfurt today?',\n",
       " 'intent_extraction': {'intent': 'current_weather',\n",
       "  'entities': {'city': 'Frankfurt', 'date': 'today'}},\n",
       " 'api_response': {'location': 'Frankfurt, DE',\n",
       "  'temperature': 12.76,\n",
       "  'description': 'clear sky',\n",
       "  'wind_speed': 7.2,\n",
       "  'humidity': 62},\n",
       " 'assistant_response': 'The weather in Frankfurt is currently clear sky with a temperature of 12.76Â°C, wind speed of 7.2 meters per second, and humidity of 62%.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f449e4b6-b5b7-43e0-93a8-c9d625701c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "intent_identification_data = []\n",
    "assistant_response_data = []\n",
    "\n",
    "for entry in weather_data:\n",
    "    # For intent identification\n",
    "    intent_identification_data.append({\n",
    "        \"user_input\": entry[\"user_input\"],\n",
    "        \"intent_extraction\": entry[\"intent_extraction\"]\n",
    "    })\n",
    "    \n",
    "    # For assistant response generation\n",
    "    assistant_response_data.append({\n",
    "        \"user_input\": entry[\"user_input\"],\n",
    "        \"api_response\": entry[\"api_response\"],\n",
    "        \"assistant_response\": entry[\"assistant_response\"]\n",
    "    })\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = intent_identification_data + assistant_response_data\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "random.shuffle(combined_data)\n",
    "# train_test_split\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "combined_dataset = Dataset.from_dict({\"conversations\": combined_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe1353c-fa76-4fd1-9944-1dadcf660c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = combined_dataset.train_test_split(test_size=0.3)  \n",
    "train_dataset = dataset_split['train']\n",
    "val_dataset = dataset_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5778f8c6-a20c-44f8-a089-959ba8c5fcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': {'api_response': None,\n",
       "  'assistant_response': None,\n",
       "  'intent_extraction': {'entities': {'city': 'Santiago', 'date': '2024-06-15'},\n",
       "   'intent': 'forecast_weather'},\n",
       "  'user_input': 'What is the forecast weather in Santiago 2024-06-15?'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3600b17-345c-41e3-b991-4b927349bc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': {'api_response': {'date': '2024-06-14',\n",
       "   'description': 'overcast clouds',\n",
       "   'humidity': 79,\n",
       "   'location': None,\n",
       "   'temperature': 11.09,\n",
       "   'wind_speed': 2.99},\n",
       "  'assistant_response': 'The forecast for Hamburg on 2024-06-14 is overcast clouds with a temperature of 11.09Â°C, wind speed of 2.99 meters per second, and humidity of 79%.',\n",
       "  'intent_extraction': None,\n",
       "  'user_input': 'What is the forecast weather in Helsinki 2024-06-12?'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3b4e65-571d-44b6-85a9-21f468a090f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4bb7b72-b528-4e4e-b319-8e24c6298e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict['intent_identification'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6593e2b7-7f15-478e-a548-2609e556852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict['assistant_response'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574abc30-4cb6-475d-8a0f-b145388ac14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929/929 [00:00<00:00, 14886.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 399/399 [00:00<00:00, 22049.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from typing import Dict, List\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"phi-3\",\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\", \"API\": \"api\"},\n",
    ")\n",
    "\n",
    "def format_combined_prompts(batch: Dict[str, List[Dict]], tokenizer):\n",
    "    def format_conversation(example: Dict[str, str]):\n",
    "        conversation = [{\"from\": \"human\", \"value\": example['user_input']}]\n",
    "        if \"intent_extraction\" in example and example['intent_extraction'] is not None:\n",
    "            conversation.append({\"from\": \"gpt\", \"value\": str(example['intent_extraction'])})\n",
    "        if \"api_response\" in example and example['api_response'] is not None:\n",
    "            conversation.append({\"from\": \"api\", \"value\": str(example['api_response'])})\n",
    "            if \"assistant_response\" in example and example['assistant_response'] is not None:\n",
    "                conversation.append({\"from\": \"gpt\", \"value\": example['assistant_response']})\n",
    "        return conversation\n",
    "\n",
    "    texts = [tokenizer.apply_chat_template(format_conversation(example), tokenize=False, add_generation_prompt=True) for example in batch[\"conversations\"]]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(lambda batch: format_combined_prompts(batch, tokenizer), batched=True)\n",
    "val_dataset = val_dataset.map(lambda batch: format_combined_prompts(batch, tokenizer), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f35072a-c06c-4daf-8e53-681dcfbeab20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': {'api_response': {'date': '2024-06-15',\n",
       "   'description': 'scattered clouds',\n",
       "   'humidity': 82,\n",
       "   'location': None,\n",
       "   'temperature': 17.95,\n",
       "   'wind_speed': 3.23},\n",
       "  'assistant_response': 'The forecast for Marseille on 2024-06-15 is scattered clouds with a temperature of 17.95Â°C, wind speed of 3.23 meters per second, and humidity of 82%.',\n",
       "  'intent_extraction': None,\n",
       "  'user_input': 'What is the current weather in Casablanca today?'},\n",
       " 'text': \"<s><|user|>\\nWhat is the current weather in Casablanca today?<|end|>\\n<|api|>\\n{'date': '2024-06-15', 'description': 'scattered clouds', 'humidity': 82, 'location': None, 'temperature': 17.95, 'wind_speed': 3.23}<|end|>\\n<|assistant|>\\nThe forecast for Marseille on 2024-06-15 is scattered clouds with a temperature of 17.95Â°C, wind speed of 3.23 meters per second, and humidity of 82%.<|end|>\\n<|assistant|>\\n\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc919810-2f1e-44f1-863d-2605c80b3b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': {'api_response': None,\n",
       "  'assistant_response': None,\n",
       "  'intent_extraction': {'entities': {'city': 'Sendai', 'date': '2024-06-14'},\n",
       "   'intent': 'forecast_weather'},\n",
       "  'user_input': 'What is the forecast weather in Sendai 2024-06-14?'},\n",
       " 'text': \"<s><|user|>\\nWhat is the forecast weather in Sendai 2024-06-14?<|end|>\\n<|assistant|>\\n{'entities': {'city': 'Sendai', 'date': '2024-06-14'}, 'intent': 'forecast_weather'}<|end|>\\n<|assistant|>\\n\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b05e5613-ae79-4778-bc38-a20510bbb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify global metric lists to include validation split\n",
    "train_metrics = {\"step\": [], \"bleu\": [], \"rouge\": [], \"perplexity\": []}\n",
    "val_metrics = {\"step\": [], \"bleu\": [], \"rouge\": [], \"perplexity\": []}\n",
    "\n",
    "def compute_metrics(eval_pred, split=\"train\"):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # BLEU\n",
    "    bleu = load_metric(\"bleu\")\n",
    "    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # ROUGE\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Perplexity\n",
    "    perplexity = torch.exp(torch.tensor(eval_pred[0].mean()))\n",
    "\n",
    "    metrics = {\n",
    "        \"bleu\": bleu_result['bleu'],\n",
    "        \"rouge\": rouge_result['rougeL'].mid.fmeasure,\n",
    "        \"perplexity\": perplexity.item(),\n",
    "    }\n",
    "\n",
    "    if split == \"train\":\n",
    "        train_metrics[\"step\"].append(len(train_metrics[\"step\"]) + 1)\n",
    "        train_metrics[\"bleu\"].append(metrics[\"bleu\"])\n",
    "        train_metrics[\"rouge\"].append(metrics[\"rouge\"])\n",
    "        train_metrics[\"perplexity\"].append(metrics[\"perplexity\"])\n",
    "    else:\n",
    "        val_metrics[\"step\"].append(len(val_metrics[\"step\"]) + 1)\n",
    "        val_metrics[\"bleu\"].append(metrics[\"bleu\"])\n",
    "        val_metrics[\"rouge\"].append(metrics[\"rouge\"])\n",
    "        val_metrics[\"perplexity\"].append(metrics[\"perplexity\"])\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b08b0c-e1c7-4c44-a70c-2f9b7a9e2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=60,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs_combined\",\n",
    "    logging_dir='./logs',\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5c7d3c5-bba8-4aec-a801-52401cf0d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48dc6600-8028-424b-8793-c973a1c2098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929/929 [00:00<00:00, 3709.22 examples/s]\n",
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 399/399 [00:00<00:00, 1731.77 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, split=\"train\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "053e55c4-eb33-441c-9f52-7dd714297b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.integration_utils.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "NotebookProgressCallback\n"
     ]
    }
   ],
   "source": [
    "# Add TensorBoard callback\n",
    "trainer.add_callback(TensorBoardCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b93a8aa-d40e-46a7-90ae-913de5db42eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 929 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 29,884,416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 03:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.060500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.975400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.601900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.275100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.820400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.638400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.349900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.300500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.289100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.287900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.209400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.261800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bce181-0a39-426b-b2ea-3d03f5c2520d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54108618-8260-4b16-8ef2-fd80c5deeb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7965239c-9f4c-4cab-836c-330e7de00647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer.py:3757\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3754\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3756\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3757\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3758\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3759\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer.py:3971\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   3970\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3971\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3972\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   3974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/accelerate/utils/operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/accelerate/utils/operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/llama.py:883\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    872\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    882\u001b[0m ):\n\u001b[0;32m--> 883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/mistral.py:213\u001b[0m, in \u001b[0;36mMistralForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m LlamaModel_fast_forward_inference(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m         input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    227\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/llama.py:681\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/llama.py:434\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    433\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 434\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/mistral.py:69\u001b[0m, in \u001b[0;36mMistralAttention_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m head_dim   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(n_kv_heads \u001b[38;5;241m*\u001b[39m n_groups \u001b[38;5;241m==\u001b[39m n_heads)\n\u001b[0;32m---> 69\u001b[0m Q, K, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_qkv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m Q \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;241m.\u001b[39mview(bsz, q_len, n_heads,    head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     71\u001b[0m K \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mview(bsz, q_len, n_kv_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:312\u001b[0m, in \u001b[0;36mapply_lora_qkv\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    310\u001b[0m KW, KW_quant, KA, KB, KS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj)\n\u001b[1;32m    311\u001b[0m VW, VW_quant, VA, VB, VS \u001b[38;5;241m=\u001b[39m get_lora_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj)\n\u001b[0;32m--> 312\u001b[0m Q, K, V \u001b[38;5;241m=\u001b[39m \u001b[43mLoRA_QKV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mKW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mVW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q, K, V\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/cuda/amp/autocast_mode.py:115\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fwd_used_autocast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     autocast_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled()\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/kernels/fast_lora.py:227\u001b[0m, in \u001b[0;36mLoRA_QKV.forward\u001b[0;34m(ctx, X, QW, QW_quant, QA, QB, QS, KW, KW_quant, KA, KB, KS, VW, VW_quant, VA, VB, VS)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mcustom_fwd\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, X : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    222\u001b[0m             QW, QW_quant, QA, QB, QS,\n\u001b[1;32m    223\u001b[0m             KW, KW_quant, KA, KB, KS,\n\u001b[1;32m    224\u001b[0m             VW, VW_quant, VA, VB, VS,):\n\u001b[1;32m    225\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 227\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQW_quant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     K \u001b[38;5;241m=\u001b[39m matmul_lora(X, KW, KW_quant, KA, KB, KS)\n\u001b[1;32m    229\u001b[0m     V \u001b[38;5;241m=\u001b[39m matmul_lora(X, VW, VW_quant, VA, VB, VS)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/kernels/utils.py:225\u001b[0m, in \u001b[0;36mmatmul_lora\u001b[0;34m(X, W, W_quant, A, B, s, out)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatmul_lora\u001b[39m(X, W, W_quant, A, B, s, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    224\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 225\u001b[0m     W \u001b[38;5;241m=\u001b[39m \u001b[43mfast_dequantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_quant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    228\u001b[0m         batch, seq_len, d \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/kernels/utils.py:89\u001b[0m, in \u001b[0;36mfast_dequantize\u001b[0;34m(W, quant_state, out)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Create weight matrix\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(out\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m shape)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa0bc527-374c-452f-87cb-9937cd3372ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_results = trainer.evaluate(eval_dataset=val_dataset, metric_key_prefix=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d78665b-4749-4929-b0f0-b3138efd64ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/VatsalPatel18/phi3-mini-WeatherBot\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"VatsalPatel18/phi3-mini-WeatherBot\", token=os.getenv(\"HUGGINGFACE_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6084716f-cc17-4b01-85c6-f5a6de12fc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/trained/tokenizer_config.json',\n",
       " '../models/trained/special_tokens_map.json',\n",
       " '../models/trained/tokenizer.model',\n",
       " '../models/trained/added_tokens.json',\n",
       " '../models/trained/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../models/trained\")\n",
    "tokenizer.save_pretrained(\"../models/trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d0e288c-05b3-48fd-895c-d877a36b2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8b59eb1-1148-4d44-ad51-9de1a3823b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 6.05 out of 14.96 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:13<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model type of mistral auto adds a BOS token.\n",
      "Unsloth: ##### If you're using Ollama or GGUF etc, do not add a BOS in the chat template.\n",
      "Unsloth: Extending ../models/gguf//tokenizer.model with added_tokens.json.\n",
      "Originally tokenizer.model is of size (32000).\n",
      "But we need to extend to sentencepiece vocab size (32011).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n",
      " \"-____-\"     In total, you will have to wait around 26 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at ../models/gguf/ into f16 GGUF format.\n",
      "The output location will be ./../models/gguf/-unsloth.F16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: gguf\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 4096\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
      "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 32000\n",
      "INFO:gguf.vocab:Setting special token type unk to 0\n",
      "INFO:gguf.vocab:Setting special token type pad to 32009\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}{% for message in messages %}{% if message['from'] == 'human' %}{{'<|user|>\n",
      "' + message['value'] + '<|end|>\n",
      "'}}{% elif message['from'] == 'gpt' %}{{'<|assistant|>\n",
      "' + message['value'] + '<|end|>\n",
      "'}}{% else %}{{'<|' + message['from'] + '|>\n",
      "' + message['value'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% endif %}\n",
      "INFO:hf-to-gguf:Exporting model to '../models/gguf/-unsloth.F16.gguf'\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {3072}\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.64G/7.64G [00:25<00:00, 295Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to '../models/gguf/-unsloth.F16.gguf'\n",
      "Unsloth: Conversion completed! Output location: ./../models/gguf/-unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 3131 (148995e5)\n",
      "main: built with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\n",
      "main: quantizing './../models/gguf/-unsloth.F16.gguf' to './../models/gguf/-unsloth.Q4_K_M.gguf' as Q4_K_M using 32 threads\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from ./../models/gguf/-unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = gguf\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32064\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 32009\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "[   1/ 291]                    token_embd.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q4_K .. size =   187.88 MiB ->    52.84 MiB\n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  20/ 291]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 291]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  22/ 291]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  23/ 291]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  24/ 291]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  25/ 291]                 blk.10.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  26/ 291]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  27/ 291]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  28/ 291]                 blk.10.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  29/ 291]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 291]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  31/ 291]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  32/ 291]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  33/ 291]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  34/ 291]                 blk.11.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  35/ 291]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  36/ 291]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  37/ 291]                 blk.11.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  38/ 291]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 291]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  40/ 291]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  41/ 291]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  42/ 291]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  43/ 291]                 blk.12.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  44/ 291]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  45/ 291]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  46/ 291]                 blk.12.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  47/ 291]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 291]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  49/ 291]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  50/ 291]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  51/ 291]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  52/ 291]                 blk.13.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  53/ 291]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  54/ 291]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  55/ 291]                 blk.13.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  56/ 291]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 291]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  58/ 291]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  59/ 291]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  60/ 291]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  61/ 291]                 blk.14.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  62/ 291]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  63/ 291]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  64/ 291]                 blk.14.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  65/ 291]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 291]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  67/ 291]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  68/ 291]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  69/ 291]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  70/ 291]                 blk.15.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  71/ 291]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  72/ 291]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  73/ 291]                 blk.15.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  74/ 291]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 291]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  76/ 291]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  77/ 291]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  78/ 291]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  79/ 291]                 blk.16.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  80/ 291]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  81/ 291]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  82/ 291]                 blk.16.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  83/ 291]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 291]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  85/ 291]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  86/ 291]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  87/ 291]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  88/ 291]                 blk.17.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  89/ 291]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  90/ 291]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  91/ 291]                 blk.17.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[  92/ 291]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 291]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  94/ 291]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  95/ 291]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  96/ 291]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  97/ 291]                 blk.18.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  98/ 291]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  99/ 291]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 100/ 291]                 blk.18.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 101/ 291]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 291]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 103/ 291]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 104/ 291]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 105/ 291]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 106/ 291]                 blk.19.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 107/ 291]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 108/ 291]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 109/ 291]                 blk.19.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 110/ 291]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 291]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 112/ 291]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 113/ 291]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 114/ 291]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 115/ 291]                  blk.2.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 116/ 291]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 117/ 291]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 118/ 291]                  blk.2.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 119/ 291]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 291]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 121/ 291]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 122/ 291]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 123/ 291]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 124/ 291]                 blk.20.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 125/ 291]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 126/ 291]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 127/ 291]                 blk.20.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 128/ 291]                 blk.21.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 129/ 291]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 130/ 291]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 131/ 291]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 132/ 291]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 133/ 291]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 134/ 291]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 135/ 291]                  blk.3.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 136/ 291]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 137/ 291]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 138/ 291]                  blk.3.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 139/ 291]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 140/ 291]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 141/ 291]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 142/ 291]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 143/ 291]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 144/ 291]                  blk.4.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 145/ 291]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 146/ 291]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 147/ 291]                  blk.4.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 148/ 291]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 149/ 291]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 150/ 291]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 151/ 291]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 152/ 291]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 153/ 291]                  blk.5.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 154/ 291]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 155/ 291]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 156/ 291]                  blk.5.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 157/ 291]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 158/ 291]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 159/ 291]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 160/ 291]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 161/ 291]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 162/ 291]                  blk.6.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 163/ 291]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 164/ 291]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 165/ 291]                  blk.6.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 166/ 291]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 167/ 291]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 168/ 291]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 169/ 291]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 170/ 291]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 171/ 291]                  blk.7.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 172/ 291]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 173/ 291]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 174/ 291]                  blk.7.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 175/ 291]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 176/ 291]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 177/ 291]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 178/ 291]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 179/ 291]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 180/ 291]                  blk.8.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 181/ 291]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 182/ 291]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 183/ 291]                  blk.8.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 184/ 291]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 185/ 291]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 186/ 291]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 187/ 291]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 188/ 291]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 189/ 291]                  blk.9.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 190/ 291]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 191/ 291]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 192/ 291]                  blk.9.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 193/ 291]                        output.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q6_K .. size =   187.88 MiB ->    77.06 MiB\n",
      "[ 194/ 291]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 291]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 196/ 291]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 197/ 291]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 198/ 291]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 199/ 291]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 200/ 291]                 blk.21.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 201/ 291]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 202/ 291]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 203/ 291]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 204/ 291]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 205/ 291]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 206/ 291]                 blk.22.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 207/ 291]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 208/ 291]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 209/ 291]                 blk.22.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 210/ 291]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 211/ 291]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 212/ 291]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 213/ 291]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 214/ 291]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 215/ 291]                 blk.23.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 216/ 291]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 217/ 291]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 218/ 291]                 blk.23.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 219/ 291]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 220/ 291]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 221/ 291]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 222/ 291]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 223/ 291]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 224/ 291]                 blk.24.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 225/ 291]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 226/ 291]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 227/ 291]                 blk.24.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 228/ 291]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 229/ 291]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 230/ 291]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 231/ 291]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 232/ 291]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 233/ 291]                 blk.25.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 234/ 291]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 235/ 291]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 236/ 291]                 blk.25.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 237/ 291]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 238/ 291]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 239/ 291]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 240/ 291]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 241/ 291]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 242/ 291]                 blk.26.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 243/ 291]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 244/ 291]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 245/ 291]                 blk.26.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 246/ 291]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 247/ 291]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 248/ 291]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 249/ 291]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 250/ 291]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 251/ 291]                 blk.27.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 252/ 291]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 253/ 291]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 254/ 291]                 blk.27.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 255/ 291]              blk.28.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 256/ 291]               blk.28.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 257/ 291]               blk.28.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 258/ 291]                 blk.28.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 259/ 291]               blk.28.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 260/ 291]                 blk.28.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 261/ 291]            blk.28.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 262/ 291]                 blk.28.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 263/ 291]                 blk.28.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 264/ 291]              blk.29.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 265/ 291]               blk.29.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 266/ 291]               blk.29.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 267/ 291]                 blk.29.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 268/ 291]               blk.29.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 269/ 291]                 blk.29.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 270/ 291]            blk.29.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 271/ 291]                 blk.29.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 272/ 291]                 blk.29.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 273/ 291]              blk.30.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 274/ 291]               blk.30.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 275/ 291]               blk.30.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 276/ 291]                 blk.30.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 277/ 291]               blk.30.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 278/ 291]                 blk.30.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 279/ 291]            blk.30.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 280/ 291]                 blk.30.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 281/ 291]                 blk.30.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 282/ 291]              blk.31.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 283/ 291]               blk.31.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 284/ 291]               blk.31.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 285/ 291]                 blk.31.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 286/ 291]               blk.31.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 287/ 291]                 blk.31.attn_k.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 288/ 291]            blk.31.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 289/ 291]                 blk.31.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 290/ 291]                 blk.31.attn_v.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q6_K .. size =    18.00 MiB ->     7.38 MiB\n",
      "[ 291/ 291]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "llama_model_quantize_internal: model size  =  7288.51 MB\n",
      "llama_model_quantize_internal: quant size  =  2210.78 MB\n",
      "\n",
      "main: quantize time = 37463.92 ms\n",
      "main:    total time = 37463.92 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model type of mistral auto adds a BOS token.\n",
      "Unsloth: ##### If you're using Ollama or GGUF etc, do not add a BOS in the chat template.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Conversion completed! Output location: ./../models/gguf/-unsloth.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\"../models/gguf/\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f81cc3b-d78f-4b0c-885d-22237242d7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_metrics(metrics, title, ylabel):\n",
    "    df = pd.DataFrame(metrics)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for column in metrics:\n",
    "        if column != \"step\":\n",
    "            plt.plot(df[\"step\"], df[column], label=column)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting training metrics\n",
    "plot_metrics(train_metrics, \"Training Metrics\", \"Score\")\n",
    "\n",
    "# Plotting validation metrics\n",
    "plot_metrics(val_metrics, \"Validation Metrics\", \"Score\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6c07711-b771-4768-bd89-e8ba2bfaee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# model = FastLanguageModel.from_pretrained(\"path/to/save/model\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"path/to/save/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0982d-2ccb-4d4d-89a0-956fa66942f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Unsloth)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
