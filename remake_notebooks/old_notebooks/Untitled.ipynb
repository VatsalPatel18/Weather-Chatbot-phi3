{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d6e734-50b7-4a87-99c3-71bdecf49837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3659f46-9cc7-412f-aa43-e9326850610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fb8d30-227a-4a8f-9818-17da95777f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherParams(BaseModel):\n",
    "    city: str = Field(..., description=\"The name of the city to get weather information for.\")\n",
    "    forecast_type: str = Field(\"current\", description=\"Either 'current' for current weather or 'forecast' for future weather.\")\n",
    "\n",
    "def get_weather(params: WeatherParams):\n",
    "    api_key = 'c6dfc4d92a8f972d237ef696ec87b37a'\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5\"\n",
    "    \n",
    "    if params.forecast_type == \"current\":\n",
    "        endpoint = f\"{base_url}/weather\"\n",
    "    else:\n",
    "        endpoint = f\"{base_url}/forecast\"\n",
    "    \n",
    "    params_dict = {\n",
    "        \"q\": params.city,\n",
    "        \"appid\": api_key,\n",
    "        \"units\": \"metric\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(endpoint, params=params_dict)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if params.forecast_type == \"current\":\n",
    "            temp = data['main']['temp']\n",
    "            description = data['weather'][0]['description']\n",
    "            return f\"The current weather in {params.city} is {description} with a temperature of {temp}°C.\"\n",
    "        else:\n",
    "            forecast = data['list'][0]  # Get the first forecast entry\n",
    "            temp = forecast['main']['temp']\n",
    "            description = forecast['weather'][0]['description']\n",
    "            date_time = forecast['dt_txt']\n",
    "            return f\"The forecast for {params.city} on {date_time} is {description} with a temperature of {temp}°C.\"\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch weather data for {params.city}. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e6109-b0e6-489f-a271-6e226618c4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d74ca49-c9cc-4379-b228-de1dbe3d52f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gguf_init_from_file: invalid magic characters 'FROM'\n",
      "llama_model_load: error loading model: llama_model_loader: failed to load model from ../models/phi-3-gguf/Modelfile_q4\n",
      "\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: ../models/phi-3-gguf/Modelfile_q4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the Llama model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/phi-3-gguf/Modelfile_q4\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update this to your actual model path\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Define the tool using LlamaCppFunctionTool\u001b[39;00m\n\u001b[1;32m     14\u001b[0m weather_function_tool \u001b[38;5;241m=\u001b[39m LlamaCppFunctionTool\u001b[38;5;241m.\u001b[39mfrom_pydantic(get_weather, WeatherParams)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp/llama.py:358\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mExitStack()\n\u001b[0;32m--> 358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack\u001b[38;5;241m.\u001b[39menter_context(contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_ \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_cpp/_internals.py:54\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[0;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfree_model\u001b[39m():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to load model from file: ../models/phi-3-gguf/Modelfile_q4"
     ]
    }
   ],
   "source": [
    "from llama_cpp_agent import LlamaCppFunctionTool, FunctionCallingAgent, MessagesFormatterType\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the Llama model\n",
    "model_path = \"../models/phi-3-gguf/Modelfile_q4\"  # Update this to your actual model path\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=0,\n",
    ")\n",
    "\n",
    "# Define the tool using LlamaCppFunctionTool\n",
    "weather_function_tool = LlamaCppFunctionTool.from_pydantic(get_weather, WeatherParams)\n",
    "\n",
    "# Create the function calling agent\n",
    "function_call_agent = FunctionCallingAgent(\n",
    "    llm,\n",
    "    llama_cpp_function_tools=[weather_function_tool],\n",
    "    send_message_to_user_callback=lambda message: print(message),\n",
    "    allow_parallel_function_calling=True,\n",
    "    messages_formatter_type=MessagesFormatterType.CHATML\n",
    ")\n",
    "\n",
    "# Define the user input\n",
    "user_input = \"What's the current weather in London?\"\n",
    "\n",
    "# Run the agent\n",
    "response = function_call_agent.generate_response(user_input)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbfef3-0ce5-48da-bc44-aba45014bac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Unsloth)",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
